{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "\n",
    "import os\n",
    "from io import open\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import BABI20\n",
    "from torchtext.data import Dataset, Field, Example, Iterator\n",
    "\n",
    "import argparse\n",
    "from torchtext import datasets\n",
    "from torchtext.datasets.babi import BABI20Field\n",
    "#from models.UTransformer import BabiUTransformer\n",
    "#from models.common_layer import NoamOpt\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mps\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Differentiable Neural Computer')\n",
    "parser.add_argument('-input_size', type=int, default=6, help='dimension of input feature')\n",
    "parser.add_argument('-rnn_type', type=str, default='lstm', help='type of recurrent cells to use for the controller')\n",
    "parser.add_argument('-nhid', type=int, default=64, help='number of hidden units of the inner nn')\n",
    "parser.add_argument('-dropout', type=float, default=0, help='controller dropout')\n",
    "parser.add_argument('-memory_type', type=str, default='dnc', help='dense or sparse memory: dnc | sdnc | sam')\n",
    "\n",
    "parser.add_argument('-nlayer', type=int, default=1, help='number of layers')\n",
    "parser.add_argument('-nhlayer', type=int, default=2, help='number of hidden layers')\n",
    "parser.add_argument('-lr', type=float, default=1e-4, help='initial learning rate')\n",
    "parser.add_argument('-optim', type=str, default='adam', help='learning rule, supports adam|rmsprop')\n",
    "parser.add_argument('-clip', type=float, default=50, help='gradient clipping')\n",
    "\n",
    "parser.add_argument('-batch_size', type=int, default=100, metavar='N', help='batch size')\n",
    "parser.add_argument('-mem_size', type=int, default=20, help='memory dimension')\n",
    "parser.add_argument('-mem_slot', type=int, default=16, help='number of memory slots')\n",
    "parser.add_argument('-read_heads', type=int, default=4, help='number of read heads')\n",
    "parser.add_argument('-sparse_reads', type=int, default=10, help='number of sparse reads per read head')\n",
    "parser.add_argument('-temporal_reads', type=int, default=2, help='number of temporal reads')\n",
    "\n",
    "parser.add_argument('-sequence_max_length', type=int, default=4, metavar='N', help='sequence_max_length')\n",
    "parser.add_argument('-curriculum_increment', type=int, default=0, metavar='N', help='sequence_max_length incrementor per 1K iterations')\n",
    "parser.add_argument('-curriculum_freq', type=int, default=1000, metavar='N', help='sequence_max_length incrementor per 1K iterations')\n",
    "parser.add_argument('-cuda', type=int, default=-1, help='Cuda GPU ID, -1 for CPU')\n",
    "\n",
    "parser.add_argument('-iterations', type=int, default=100000, metavar='N', help='total number of iteration')\n",
    "parser.add_argument('-summarize_freq', type=int, default=100, metavar='N', help='summarize frequency')\n",
    "parser.add_argument('-check_freq', type=int, default=100, metavar='N', help='check point frequency')\n",
    "parser.add_argument('-visdom', action='store_true', help='plot memory content on visdom per -summarize_freq steps')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMBABI(nn.Module):\n",
    "    # モデルで使う各ネットワークをコンストラクタで定義\n",
    "    def __init__(self, input_dim, hidden_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(MyLSTMBABI, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.flag=True\n",
    "    # 順伝播処理はforward関数に記載\n",
    "    def forward(self, sentences,hidden=None):\n",
    "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値＝hiddenだけ使う。　babiもmany to oneのはず\n",
    "        sentences = sentences.permute(1,0,2) #(seq_len,batch, vocab_size)\n",
    "\n",
    "        _, lstm_out = self.lstm(sentences)  # _ は(seq_len, batch, hidden_dim)\n",
    "        \n",
    "        \n",
    "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
    "        tag_space = self.hidden2tag(lstm_out[0].view(sentences.size()[1], self.hidden_dim)) #(batch, hidden_dim)にしてから食わせる\n",
    "        \"\"\"if self.flag:\n",
    "            print(\"linear out\")\n",
    "            print(tag_space)\n",
    "            self.flag=False\"\"\"\n",
    "        tag_score =self.softmax(tag_space)\n",
    "        return tag_score #(batch, vocab_size) のはず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_criterion(predictions, targets):\n",
    "  return T.mean(\n",
    "      -1 * F.logsigmoid(predictions) * (targets) - T.log(1 - F.sigmoid(predictions) + 1e-9) * (1 - targets)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_babidata(config):\n",
    "    \"\"\"(batch_size=config.batch_size, \n",
    "                                                            root='.data', \n",
    "                                                            memory_size=70, \n",
    "                                                            task=config.task, \n",
    "                                                            joint=False,\n",
    "                                                            tenK=False, \n",
    "                                                            only_supporting=False, \n",
    "                                                            sort=False, \n",
    "                                                            shuffle=True)\"\"\"\n",
    "    text = BABI20Field(50)\n",
    "    train, val, test = datasets.BABI20.splits(text, root='.data', task=2, joint=False,\n",
    "                                            tenK=True, only_supporting=False)\n",
    "    text.build_vocab(train)\n",
    "    vocab_len1 = len(text.vocab.freqs) \n",
    "    print(\"VOCAB LEN:\",vocab_len1 )\n",
    "    train_iter,val_iter,test_iter = Iterator.splits((train, val, test),batch_size=32)\n",
    "    return train_iter, val_iter, test_iter,vocab_len1+1,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_babidata_task(task):\n",
    "    \"\"\"(batch_size=config.batch_size, \n",
    "                                                            root='.data', \n",
    "                                                            memory_size=70, \n",
    "                                                            task=config.task, \n",
    "                                                            joint=False,\n",
    "                                                            tenK=False, \n",
    "                                                            only_supporting=False, \n",
    "                                                            sort=False, \n",
    "                                                            shuffle=True)\"\"\"\n",
    "    text = BABI20Field(50)\n",
    "    train, val, test = datasets.BABI20.splits(text, root='.data', task=task, joint=False,\n",
    "                                            tenK=True, only_supporting=False)\n",
    "    text.build_vocab(train)\n",
    "    #text.vocab.append_token\n",
    "    vocab_len = len(text.vocab.freqs) +1\n",
    "    vocab_lenplus =vocab_len+1 # \"?\"\n",
    "    print(\"VOCAB LEN PLUS:\",vocab_lenplus )\n",
    "    train_iter,val_iter,test_iter = Iterator.splits((train, val, test),batch_size=32)\n",
    "    return train_iter, val_iter, test_iter,vocab_lenplus,text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    babi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangestory(batch_story): # (batch,length,size)\n",
    "    strysum =torch.sum(batch_story,(0,2)) #1D\n",
    "    fill_length=len(strysum[torch.where(strysum!=0)])\n",
    "    red_story=batch_story[:,:fill_length,:]\n",
    "    red_story =torch.flip(red_story, dims=[1])\n",
    "    flat_story =red_story.view(len(batch_story),-1)\n",
    "    return flat_story #(batch, red_length * size ) 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(batch,vocab_size):\n",
    "    batch_size=batch.story.size()[0]\n",
    "    flat_story =arrangestory(batch.story)\n",
    "    story_OH =torch.nn.functional.one_hot(flat_story,num_classes=vocab_size) #(batch, seq_len, vocab_len)\n",
    "    query_OH=torch.nn.functional.one_hot(batch.query,num_classes=vocab_size) #(batch, que_len=3, vocab_len)\n",
    "    answer_OH=torch.nn.functional.one_hot(batch.answer,num_classes=vocab_size) #(batch,1, vocab_len)\n",
    "    querystop =torch.nn.functional.one_hot(torch.tensor([vocab_size-1]*batch_size),num_classes=vocab_size)\n",
    "    querystop=querystop.view(batch_size,1,-1) #2D -> 3D\n",
    "    x =torch.cat((story_OH,query_OH,querystop),1)\n",
    "    y =answer_OH.view(batch_size,vocab_size)\n",
    "\n",
    "    x = var(x.to(torch.float).cuda() )\n",
    "    y = var(y.to(torch.float).cuda() )\n",
    "    ans_id =batch.answer.view(-1) #1D (batch_size)\n",
    "    #ans_id=var(ans_id.to(torch.float).cuda() )\n",
    "    ans_id=var(ans_id.cuda() )\n",
    "\n",
    "    return x,y,ans_id #yはonehotなので2Dだけどans_idは1D !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDtensor2word(tns,text_field):\n",
    "  itos_list=text_field.vocab.itos\n",
    "  id_np =tns.to('cpu').detach().numpy().copy()\n",
    "  shp =id_np.shape\n",
    "  id_np=id_np.flatten()\n",
    "  word_np =[itos_list[i] for i in id_np]\n",
    "  word_np=np.reshape(word_np,shp)\n",
    "  return word_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxORonehot2word(tns,text_field):\n",
    "    #ans_id =batch.answer.view(-1) #1D (batch_size)\n",
    "    tns_dim=tns.dim()\n",
    "    print(type(tns_dim))\n",
    "    tns_id = torch.argmax(tns,tns_dim-1)\n",
    "    return IDtensor2word(tns_id,text_field)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/field.py:150: UserWarning: BABI20Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB LEN PLUS: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter, vocab_size, text_field =get_babidata_task(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['John' 'travelled' 'to' 'the' 'bedroom' '<pad>']\n",
      "  ['Sandra' 'travelled' 'to' 'the' 'garden' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']]\n",
      "\n",
      " [['Daniel' 'moved' 'to' 'the' 'office' '<pad>']\n",
      "  ['John' 'journeyed' 'to' 'the' 'hallway' '<pad>']\n",
      "  ['Daniel' 'travelled' 'to' 'the' 'hallway' '<pad>']\n",
      "  ['John' 'journeyed' 'to' 'the' 'bathroom' '<pad>']\n",
      "  ['Sandra' 'moved' 'to' 'the' 'bathroom' '<pad>']\n",
      "  ['Mary' 'went' 'back' 'to' 'the' 'office']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
      "  ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']]]\n",
      "[['Where' 'is' 'John']\n",
      " ['Where' 'is' 'Sandra']]\n",
      "[['bedroom']\n",
      " ['bathroom']]\n",
      "torch.Size([32, 50, 6])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 1])\n",
      "10\n",
      "torch.Size([32, 10, 6])\n",
      "torch.Size([32, 60])\n",
      "[['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' 'Sandra' 'travelled' 'to' 'the' 'garden'\n",
      "  '<pad>' 'John' 'travelled' 'to' 'the' 'bedroom' '<pad>']\n",
      " ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' 'Mary' 'went' 'back'\n",
      "  'to' 'the' 'office' 'Sandra' 'moved' 'to' 'the' 'bathroom' '<pad>'\n",
      "  'John' 'journeyed' 'to' 'the' 'bathroom' '<pad>' 'Daniel' 'travelled'\n",
      "  'to' 'the' 'hallway' '<pad>' 'John' 'journeyed' 'to' 'the' 'hallway'\n",
      "  '<pad>' 'Daniel' 'moved' 'to' 'the' 'office' '<pad>']]\n",
      "[['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' 'Sandra' 'travelled' 'to' 'the' 'garden'\n",
      "  '<pad>' 'John' 'travelled' 'to' 'the' 'bedroom' '<pad>' 'Where' 'is'\n",
      "  'John']\n",
      " ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' 'Mary' 'went' 'back'\n",
      "  'to' 'the' 'office' 'Sandra' 'moved' 'to' 'the' 'bathroom' '<pad>'\n",
      "  'John' 'journeyed' 'to' 'the' 'bathroom' '<pad>' 'Daniel' 'travelled'\n",
      "  'to' 'the' 'hallway' '<pad>' 'John' 'journeyed' 'to' 'the' 'hallway'\n",
      "  '<pad>' 'Daniel' 'moved' 'to' 'the' 'office' '<pad>' 'Where' 'is'\n",
      "  'Sandra']\n",
      " ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' 'Mary' 'went' 'to' 'the' 'garden' '<pad>'\n",
      "  'John' 'travelled' 'to' 'the' 'kitchen' '<pad>' 'Where' 'is' 'John']\n",
      " ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' 'John' 'travelled' 'to' 'the' 'bedroom' '<pad>'\n",
      "  'John' 'moved' 'to' 'the' 'hallway' '<pad>' 'Where' 'is' 'John']]\n",
      "torch.Size([32, 64, 21])\n",
      "torch.Size([32, 21])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4, 10,  2,  1, 14,  0,\n",
      "          5, 10,  2,  1, 15,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  6,  3,  9,  2,  1, 17,  4, 12,  2,  1, 13,  0,\n",
      "          5,  8,  2,  1, 13,  0,  7, 10,  2,  1, 11,  0,  5,  8,  2,  1, 11,  0,\n",
      "          7, 12,  2,  1, 17,  0]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4, 10,  2,  1, 14,  0,\n",
      "          5, 10,  2,  1, 15,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  6,  3,  9,  2,  1, 17,  4, 12,  2,  1, 13,  0,\n",
      "          5,  8,  2,  1, 13,  0,  7, 10,  2,  1, 11,  0,  5,  8,  2,  1, 11,  0,\n",
      "          7, 12,  2,  1, 17,  0]])\n",
      "<class 'int'>\n",
      "[['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' 'Sandra' 'travelled' 'to' 'the' 'garden'\n",
      "  '<pad>' 'John' 'travelled' 'to' 'the' 'bedroom' '<pad>' 'Where' 'is'\n",
      "  'John']\n",
      " ['<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>'\n",
      "  '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' 'Mary' 'went' 'back'\n",
      "  'to' 'the' 'office' 'Sandra' 'moved' 'to' 'the' 'bathroom' '<pad>'\n",
      "  'John' 'journeyed' 'to' 'the' 'bathroom' '<pad>' 'Daniel' 'travelled'\n",
      "  'to' 'the' 'hallway' '<pad>' 'John' 'journeyed' 'to' 'the' 'hallway'\n",
      "  '<pad>' 'Daniel' 'moved' 'to' 'the' 'office' '<pad>' 'Where' 'is'\n",
      "  'Sandra']]\n",
      "<class 'int'>\n",
      "(32, 63)\n",
      "<class 'int'>\n",
      "['bedroom' 'bathroom']\n",
      "<class 'int'>\n",
      "(32,)\n",
      "64\n",
      "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for epoch,batch in enumerate(train_iter):\n",
    "    batch_size=batch.batch_size\n",
    "    #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "    batch_story=batch.story\n",
    "    print(IDtensor2word(batch_story[:2],text_field))\n",
    "    print(IDtensor2word(batch.query[:2],text_field))\n",
    "    print(IDtensor2word(batch.answer[:2],text_field))\n",
    "    print(batch_story.size())\n",
    "    print(batch.query.size())\n",
    "    print(batch.answer.size())\n",
    "\n",
    "    strysum =torch.sum(batch_story,(0,2)) #1D\n",
    "    fill_length=len(strysum[torch.where(strysum!=0)])\n",
    "    red_story=batch_story[:,:fill_length,:]\n",
    "    red_story =torch.flip(red_story, dims=[1])\n",
    "    flat_story =red_story.view(len(batch_story),-1)\n",
    "    print(fill_length)\n",
    "    print(red_story.size())\n",
    "    print(flat_story.size())\n",
    "    print(IDtensor2word(flat_story[:2],text_field))\n",
    "    print(IDtensor2word(torch.cat((flat_story[:4],batch.query[:4]),1),text_field))\n",
    "\n",
    "    story_OH =torch.nn.functional.one_hot(flat_story,num_classes=vocab_size) #(batch, seq_len, vocab_len)\n",
    "    query_OH=torch.nn.functional.one_hot(batch.query,num_classes=vocab_size) #(batch, que_len=3, vocab_len)\n",
    "    answer_OH=torch.nn.functional.one_hot(batch.answer,num_classes=vocab_size) #(batch,1, vocab_len)\n",
    "    querystop =torch.nn.functional.one_hot(torch.tensor([vocab_size-1]*batch_size),num_classes=vocab_size)\n",
    "    querystop=querystop.view(batch_size,1,-1) #2D -> 3D\n",
    "    train_x =torch.cat((story_OH,query_OH,querystop),1)\n",
    "    train_y =answer_OH.view(batch_size,-1)\n",
    "    \n",
    "    train_x = var(train_x.to(torch.float).cuda() )\n",
    "    train_y = var(train_y.to(torch.float).cuda() )\n",
    "\n",
    "    print(train_x.size())\n",
    "    print(train_y.size())\n",
    "    #argmaxによるone-hot再変換でidが変わってないか確認\n",
    "    print(flat_story[:2])\n",
    "    print(torch.argmax(torch.nn.functional.one_hot(flat_story[:2],num_classes=vocab_size) ,2))\n",
    "    \n",
    "    #one-hotをargmaxで復元するのが間違ってないか確認\n",
    "    print(softmaxORonehot2word(torch.cat((story_OH,query_OH),1)[:2],text_field))\n",
    "    print(softmaxORonehot2word(torch.cat((story_OH,query_OH),1),text_field).shape)\n",
    "    print(softmaxORonehot2word(train_y[:2],text_field))\n",
    "    print(softmaxORonehot2word(train_y,text_field).shape)\n",
    "\n",
    "    seq_len=train_x.size()[1]\n",
    "    print(seq_len)\n",
    "\n",
    "    print(querystop)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"config = parse_config(argtext=\"--task 4 --batch_size 32 --cuda --verbose\")\n",
    "\n",
    "for t in range(1,21):\n",
    "    config.task = t\n",
    "    acc = []\n",
    "    for i in range(config.run_avg):\"\"\"\n",
    "\n",
    "def main_lstm(task):\n",
    "    lr=0.005\n",
    "    print(\"==========TASK: \",task,\" learning_rate:\",lr)\n",
    "    train_iter, val_iter, test_iter, vocab_size, text_field =get_babidata_task(task)\n",
    "    # model generate, optimizer and criterion setting\n",
    "    model= MyLSTMBABI(input_dim=vocab_size, hidden_dim=512, tagset_size=vocab_size).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    save_dir=\"lstm_param\"\n",
    "    save_path=os.path.join(save_dir,\"task\"+str(task)+\".pth\")\n",
    "\n",
    "    iterations=40\n",
    "    summarize_freq=5\n",
    "    last_save_losses = []\n",
    "    last_val_losses =[]\n",
    "    val_acc=[]\n",
    "    val_bestacc=0\n",
    "    val_bestloss=0\n",
    "    bestepoch =0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(iterations):\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "            batch_size=batch.batch_size\n",
    "            train_x,train_y,ans_id =prep_data(batch,vocab_size)\n",
    "\n",
    "            output = model(train_x)\n",
    "\n",
    "            optimizer.zero_grad()    \n",
    "            loss = criterion(output,ans_id)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_value = loss.item()\n",
    "            last_save_losses.append(loss_value)\n",
    "        \n",
    "        #validation\n",
    "        with torch.no_grad():\n",
    "            for i,batch in enumerate(val_iter):\n",
    "                #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "                batch_size=batch.batch_size\n",
    "                val_x,val_y,val_ansid =prep_data(batch,vocab_size)\n",
    "                seq_len=val_x.size()[1]\n",
    "\n",
    "                output = model(val_x)\n",
    "\n",
    "                optimizer.zero_grad()    \n",
    "                loss = criterion(output,val_ansid)\n",
    "                loss_value = loss.item()\n",
    "                last_val_losses.append(loss_value)\n",
    "\n",
    "                pred_id = torch.argmax(output,1) #1D\n",
    "                pred_id = pred_id.cpu().numpy() # (batch_size, vocab_len)　のはず\n",
    "                val_ansid = val_ansid.cpu().numpy()\n",
    "                val_acc.append(((pred_id == val_ansid).sum()/ len(val_ansid) ))\n",
    "\n",
    "        summarize = ((epoch+1) % summarize_freq == 0)\n",
    "        if summarize:\n",
    "            loss = np.mean(last_save_losses)\n",
    "            print(\"epoch:\",epoch,\" loss:\",loss)\n",
    "            val_loss = np.mean(last_val_losses)\n",
    "            print(\"    val_loss:\",val_loss)\n",
    "            mean_acc =np.mean(val_acc)\n",
    "            print(\"    val_acc:\",mean_acc)\n",
    "            if mean_acc>val_bestacc:\n",
    "                val_bestacc =mean_acc\n",
    "                val_bestloss=val_loss\n",
    "                bestepoch=epoch\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            last_save_losses = []\n",
    "            last_val_losses =[]\n",
    "            val_acc=[]\n",
    "    print(\"\\n val best accuracy: \",val_bestacc,\" epoch: \",bestepoch)\n",
    "\n",
    "    #Test\n",
    "    print(\"=====TEST=====\")\n",
    "\n",
    "    last_save_losses=[]\n",
    "    accuracy_rates =[]\n",
    "    test_loss=0\n",
    "    test_acc=0\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(test_iter):\n",
    "            #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "            batch_size=batch.batch_size\n",
    "            test_x,test_y,ans_id =prep_data(batch,vocab_size)\n",
    "            seq_len=test_x.size()[1]\n",
    "\n",
    "            output = model(test_x)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            #loss計算   \n",
    "            loss = criterion(output,ans_id)\n",
    "            loss_value = loss.item()\n",
    "            last_save_losses.append(loss_value)\n",
    "\n",
    "            #予測に変換\n",
    "            \"\"\"if output.dim()!=2:\n",
    "                print(\"error\")\"\"\"\n",
    "            pred_id = torch.argmax(output,1) #1D\n",
    "            \n",
    "            #print(\"accuracy rate:\",((pred_id == ans_id).sum().float() / len(ans_id) ).item() ) #32バッチ内での正答率\n",
    "            pred_id = pred_id.cpu().numpy() # (batch_size, vocab_len)　のはず\n",
    "            ans_id = ans_id.cpu().numpy()\n",
    "            accuracy_rates.append(((pred_id == ans_id).sum()/ len(ans_id) ))\n",
    "            #numpy型にしたのでfloatへの変換は必要なし\n",
    "\n",
    "        test_loss = np.mean(last_save_losses)\n",
    "        test_acc =np.mean(accuracy_rates)\n",
    "        print(\" loss:\",test_loss)\n",
    "        print(\"accuracy_rate:\",test_acc)\n",
    "\n",
    "    return {\"task\":task,\"best_epoch\":bestepoch,\n",
    "    \"best_val_loss\":val_bestloss,\"best_val_acc\":val_bestacc,\n",
    "    \"test_loss\":test_loss,\"test_acc\":test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========TASK:  5  learning_rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/field.py:150: UserWarning: BABI20Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB LEN PLUS: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4  loss: 1.5878346913249781\n",
      "    val_loss: 1.5246030598878861\n",
      "    val_acc: 0.24453125\n",
      "epoch: 9  loss: 1.4876092597947899\n",
      "    val_loss: 1.5502733200788499\n",
      "    val_acc: 0.2849609375\n",
      "epoch: 14  loss: 1.3135680327178738\n",
      "    val_loss: 1.3495812237262725\n",
      "    val_acc: 0.3482421875\n",
      "epoch: 19  loss: 1.2810428208493172\n",
      "    val_loss: 1.3390481486916541\n",
      "    val_acc: 0.350390625\n",
      "epoch: 24  loss: 1.2577998370988994\n",
      "    val_loss: 1.34085821993649\n",
      "    val_acc: 0.3650390625\n",
      "epoch: 29  loss: 1.2339473258519003\n",
      "    val_loss: 1.3494028739631176\n",
      "    val_acc: 0.3720703125\n",
      "epoch: 34  loss: 1.2174526939155361\n",
      "    val_loss: 1.3717475671321153\n",
      "    val_acc: 0.3537109375\n",
      "epoch: 39  loss: 1.2025334834629762\n",
      "    val_loss: 1.3369005836546421\n",
      "    val_acc: 0.3767578125\n",
      "\n",
      " val best accuracy:  0.3767578125  epoch:  39\n",
      "=====TEST=====\n",
      " loss: 1.3201708868145943\n",
      "accuracy_rate: 0.359375\n",
      "   task  best_epoch  best_val_loss  best_val_acc  test_loss  test_acc\n",
      "0     5          39       1.336901      0.376758   1.320171  0.359375\n"
     ]
    }
   ],
   "source": [
    "state=[]\n",
    "for task in range(5,6):\n",
    "    state .append(main_lstm(task))\n",
    "\n",
    "print(pd.DataFrame(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6926040649414062\n",
      "epoch: 50  loss: 0.17907388955354692\n",
      "epoch: 100  loss: 0.12981969431042673\n",
      "epoch: 150  loss: 0.13114346712827682\n",
      "epoch: 200  loss: 0.13026186019182207\n",
      "epoch: 250  loss: 0.13087139889597893\n"
     ]
    }
   ],
   "source": [
    "#1timestepずつ入れてた残骸\n",
    "\"\"\"config = parse_config(argtext=\"--task 4 --batch_size 32 --cuda --verbose\")\n",
    "\n",
    "for t in range(1,21):\n",
    "    config.task = t\n",
    "    acc = []\n",
    "    for i in range(config.run_avg):\"\"\"\n",
    "\n",
    "# model generate, optimizer and criterion setting\n",
    "#vocab_size=\n",
    "\n",
    "model= MyLSTMCopyFirst(input_dim=vocab_size, hidden_dim=512, tagset_size=vocab_size).cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#Learn\n",
    "summarize_freq=50\n",
    "last_save_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch,batch in enumerate(train_iter):\n",
    "    batch_size=batch.batch_size\n",
    "    #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "    flat_story =arrangestory(batch.story)\n",
    "    story_OH =torch.nn.functional.one_hot(flat_story,num_classes=vocab_size) #(batch, seq_len, vocab_len)\n",
    "    query_OH=torch.nn.functional.one_hot(batch.query,num_classes=vocab_size) #(batch, que_len=3, vocab_len)\n",
    "    answer_OH=torch.nn.functional.one_hot(batch.answer,num_classes=vocab_size) #(batch,1, vocab_len)\n",
    "    querystop =torch.nn.functional.one_hot(torch.tensor([vocab_size-1]*batch_size),num_classes=vocab_size)\n",
    "    querystop=querystop.view(batch_size,1,-1) #2D -> 3D\n",
    "    train_x =torch.cat((story_OH,query_OH,querystop),1)\n",
    "    train_y =answer_OH.view(batch_size,-1)\n",
    "\n",
    "    train_x = var(train_x.to(torch.float).cuda() )\n",
    "    train_y = var(train_y.to(torch.float).cuda() )\n",
    "    seq_len=train_x.size()[1]\n",
    "\n",
    "    hidden=None\n",
    "    for time in range(seq_len): #DataLoader　ではないと思う。layer forwardにあたる\n",
    "        sentence =train_x[:,time,:]\n",
    "\n",
    "        if time==0:\n",
    "            output,hidden = model(sentence)\n",
    "        else:\n",
    "            output,hidden = model(sentence,hidden)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #最後のステップだけloss計算する\n",
    "        if (time==seq_len-1):\n",
    "            loss = my_criterion((output),train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_value = loss.item()\n",
    "            last_save_losses.append(loss_value)\n",
    "    hidden=None\n",
    "    summarize = (epoch % summarize_freq == 0)\n",
    "    if summarize:\n",
    "        loss = np.mean(last_save_losses)\n",
    "        print(\"epoch:\",epoch,\" loss:\",loss)\n",
    "        last_save_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for bb in test_iter:\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31  loss: 0.13347184797748923\n",
      "accuracy_rate: 0.1484375\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "last_save_losses=[]\n",
    "accuracy_rates =[]\n",
    "\n",
    "epoch_size =32\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for epoch,batch in enumerate(test_iter):\n",
    "        batch_size=batch.batch_size\n",
    "        #print(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "        flat_story =arrangestory(batch.story)\n",
    "        story_OH =torch.nn.functional.one_hot(flat_story,num_classes=vocab_size) #(batch, seq_len, vocab_len)\n",
    "        query_OH=torch.nn.functional.one_hot(batch.query,num_classes=vocab_size) #(batch, que_len=3, vocab_len)\n",
    "        answer_OH=torch.nn.functional.one_hot(batch.answer,num_classes=vocab_size) #(batch,1, vocab_len)\n",
    "        querystop =torch.nn.functional.one_hot(torch.tensor([vocab_size-1]*batch_size),num_classes=vocab_size)\n",
    "        querystop=querystop.view(batch_size,1,-1) #2D -> 3D\n",
    "        test_x =torch.cat((story_OH,query_OH,querystop),1)\n",
    "        test_y =answer_OH.view(batch_size,-1) #3D->2D\n",
    "        test_x = var(test_x.to(torch.float).cuda() )\n",
    "        test_y = var(test_y.to(torch.float).cuda() )\n",
    "        seq_len=test_x.size()[1]\n",
    "\n",
    "        hidden=None\n",
    "        for time in range(seq_len): #\n",
    "            sentence =test_x[:,time,:]\n",
    "\n",
    "            if time==0:\n",
    "                output,hidden = model(sentence)\n",
    "            else:\n",
    "                output,hidden = model(sentence,hidden)\n",
    "            \n",
    "            #最後のステップだけloss計算する\n",
    "            if (time==seq_len-1):\n",
    "                loss = my_criterion((output),test_y)\n",
    "                loss_value = loss.item()\n",
    "                last_save_losses.append(loss_value)\n",
    "\n",
    "                ans_id =batch.answer.view(-1) #1D (batch_size)\n",
    "                ans_id=var(ans_id.to(torch.float).cuda() )\n",
    "                pred_id = torch.argmax(output,1)\n",
    "\n",
    "                output = output.cpu().numpy() # (batch_size, vocab_len)　のはず\n",
    "                test_y = test_y.cpu().numpy()\n",
    "\n",
    "                #print(\"accuracy rate:\",((pred_id == ans_id).sum().float() / len(ans_id) ).item() ) #32バッチ内での正答率\n",
    "                accuracy_rates.append(((pred_id == ans_id).sum().float() / len(ans_id) ).item())\n",
    "\n",
    "        hidden=None\n",
    "        summarize =(epoch==epoch_size-1)\n",
    "        if summarize:\n",
    "            loss = np.mean(last_save_losses)\n",
    "            print(\"epoch:\",epoch,\" loss:\",loss)\n",
    "            print(\"accuracy_rate:\",np.mean(accuracy_rates))\n",
    "            last_save_losses = []\n",
    "            #print(\"test_loss:\",loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiddenとoutputのサイズが共通な以上、全結合層を挟まないとhidden_sizeが分類クラス数あるいはvocab_sizeに固定されてしまうよ　　\n",
    "扱うのがコピーから文章になると、embedding層やvocab_size引数が必要になるよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMClassifier(nn.Module):\n",
    "    # モデルで使う各ネットワークをコンストラクタで定義\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(MyLSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # 順伝播処理はforward関数に記載\n",
    "    def forward(self, sentence):\n",
    "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
    "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値＝hiddenだけ使う。　babiもmany to oneのはず\n",
    "        _, lstm_out = self.lstm(sentence.view(len(sentence), 1, -1))\n",
    "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
    "        tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        # softmaxに食わせて、確率として表現\n",
    "        tag_scores = self.softmax(tag_space)\n",
    "        return tag_scores\n",
    "        #hiddenは明示的にforwardの入力にしなくても大丈夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'embedding_dim', 'hidden_dim', 'vocab_size', and 'tagset_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model generate, optimizer and criterion setting\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mMyLSTMClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'embedding_dim', 'hidden_dim', 'vocab_size', and 'tagset_size'"
     ]
    }
   ],
   "source": [
    "# model generate, optimizer and criterion setting\n",
    "\n",
    "model= MyLSTMClassifier().cuda()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn\n",
    "\n",
    "n =     ##  データのサイズ\n",
    "bs =    ##  バッチのサイズ\n",
    "itr=5\n",
    "\n",
    "model.train()\n",
    "for i in range(itr):\n",
    "    idx = np.random.permutation(n)\n",
    "    for j in range(0,n,bs):\n",
    "        xtm = xtrain[idx[j:(j+bs) if (j+bs)<n else n]]\n",
    "        ytm = ytrain[idx[j:(j+bs) if (j+bs) < n else n]]\n",
    "        output = model(xtm)\n",
    "        loss = criterion(output,ytm)\n",
    "        print(i,j,loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output1= model(xtest)\n",
    "    ans = torch.argmax(output1,1)\n",
    "    print(((ytest == ans).sum().float() / len(ans) ).item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓は文章をあつかうやつ　コピーの次に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "class MyLSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        self.l1=nn.Linear(4,6)\n",
    "        self.l2=nn.Linear(6,3)\n",
    "    def forward(self,x):\n",
    "        h1=torch.sigmoid(self.l1(x))\n",
    "        h2=self.l2(h1)\n",
    "        return h2\n",
    "\n",
    "class MyLSTMClassifier(nn.Module):\n",
    "    # モデルで使う各ネットワークをコンストラクタで定義\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(MyLSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # インプットの単語をベクトル化するために使う\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # 順伝播処理はforward関数に記載\n",
    "    def forward(self, sentence):\n",
    "        # 文章内の各単語をベクトル化して出力。2次元のテンソル\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
    "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
    "        _, lstm_out = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
    "        tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        # softmaxに食わせて、確率として表現\n",
    "        tag_scores = self.softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfirstをbabiに適用しようとした残骸\n",
    "class MyLSTMBABItest(nn.Module):\n",
    "    # モデルで使う各ネットワークをコンストラクタで定義\n",
    "    def __init__(self, input_dim, hidden_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(MyLSTMBABI, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.flag=True\n",
    "    # 順伝播処理はforward関数に記載\n",
    "    def forward(self, sentence,hidden=None):\n",
    "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
    "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値＝hiddenだけ使う。　babiもmany to oneのはず\n",
    "\n",
    "        if (hidden==None):\n",
    "            _, lstm_out = self.lstm(sentence.view(1,len(sentence),  -1)) #(1,batch, size)\n",
    "        else:\n",
    "            _,lstm_out = self.lstm(sentence.view(1,len(sentence),  -1),hidden)\n",
    "        \n",
    "        #sentence = sentence.permute(1,0,2)\n",
    "        # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
    "        #if self.flag:\n",
    "        #    print(\"lstm_out[0] reshape\")\n",
    "        #    print(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        \"\"\"if self.flag:\n",
    "            print(\"linear out\")\n",
    "            print(tag_space)\n",
    "            self.flag=False\"\"\"\n",
    "        #tag_space=tag_space.view(len(sentence),1,-1)\n",
    "        return tag_space,lstm_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

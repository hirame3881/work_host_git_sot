{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "PATH_CUR = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "sys.path.append(PATH_CUR)\n",
    "sys.path.append('../')\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from babi.data_preprocess.preprocess import parse\n",
    "from baselines.sam import qamodel\n",
    "from baselines.sam.utils import WarmupScheduler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: Dict[str, Dict],\n",
    "          serialization_path: str,\n",
    "          eval_test: bool = False,\n",
    "          force: bool = False) -> None:\n",
    "    # Create serialization dir\n",
    "    dir_path = Path(serialization_path)\n",
    "    print(dir_path)\n",
    "    if dir_path.exists() and force:\n",
    "        shutil.rmtree(dir_path)\n",
    "    if not dir_path.exists():\n",
    "        dir_path.mkdir(parents=True, exist_ok=False)\n",
    "    model_path = dir_path / \"model.pt\"\n",
    "    config_path = dir_path / \"config.json\"\n",
    "    writer = SummaryWriter(log_dir=str(dir_path))\n",
    "\n",
    "    # Read config\n",
    "    data_config = config[\"data\"]\n",
    "    trainer_config = config[\"trainer\"]\n",
    "    model_config = config[\"model\"]\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    # Load data\n",
    "    if data_config[\"task-id\"]==\"all\":\n",
    "        task_ids = range(1,21)\n",
    "    else:\n",
    "        task_ids = [data_config[\"task-id\"]]\n",
    "    # train_raw_data, valid_raw_data, test_raw_data, word2id = parse_all(data_config[\"data_path\"],list(range(1,21)))\n",
    "    word2id = None\n",
    "    train_data_loaders = {}\n",
    "    valid_data_loaders = {}\n",
    "    test_data_loaders = {}\n",
    "\n",
    "    num_train_batches = num_valid_batches = num_test_batches = 0\n",
    "    max_seq = 0\n",
    "    for i in task_ids:\n",
    "        train_raw_data, valid_raw_data, test_raw_data, word2id = parse(data_config[\"data_path\"],\n",
    "                                                                       str(i), word2id=word2id,\n",
    "                                                                       use_cache=True, cache_dir_ext=\"\")\n",
    "        train_epoch_size = train_raw_data[0].shape[0]\n",
    "        valid_epoch_size = valid_raw_data[0].shape[0]\n",
    "        test_epoch_size = test_raw_data[0].shape[0]\n",
    "\n",
    "        max_story_length = np.max(train_raw_data[1])\n",
    "        max_sentences = train_raw_data[0].shape[1]\n",
    "        max_seq = max(max_seq, train_raw_data[0].shape[2])\n",
    "        max_q = train_raw_data[0].shape[1]\n",
    "        valid_batch_size = valid_epoch_size // 73  # like in the original implementation\n",
    "        test_batch_size = test_epoch_size // 73\n",
    "\n",
    "\n",
    "\n",
    "        train_dataset = TensorDataset(*[torch.LongTensor(a) for a in train_raw_data[:-1]])\n",
    "        valid_dataset = TensorDataset(*[torch.LongTensor(a) for a in valid_raw_data[:-1]])\n",
    "        test_dataset = TensorDataset(*[torch.LongTensor(a) for a in test_raw_data[:-1]])\n",
    "\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=trainer_config[\"batch_size\"], shuffle=True)\n",
    "        valid_data_loader = DataLoader(valid_dataset, batch_size=valid_batch_size)\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "        train_data_loaders[i] = [iter(train_data_loader), train_data_loader]\n",
    "        valid_data_loaders[i] = valid_data_loader\n",
    "        test_data_loaders[i] = test_data_loader\n",
    "\n",
    "        num_train_batches += len(train_data_loader)\n",
    "        num_valid_batches += len(valid_data_loader)\n",
    "        num_test_batches += len(test_data_loader)\n",
    "\n",
    "    print(f\"total train data: {num_train_batches*trainer_config['batch_size']}\")\n",
    "    print(f\"total valid data: {num_valid_batches*valid_batch_size}\")\n",
    "    print(f\"total test data: {num_test_batches*test_batch_size}\")\n",
    "    print(f\"voca size {len(word2id)}\")\n",
    "\n",
    "    model_config[\"vocab_size\"] = len(word2id)\n",
    "    model_config[\"max_seq\"] = max_seq\n",
    "    model_config[\"symbol_size\"] = 64\n",
    "    # Create model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = qamodel.QAmodel(model_config).to(device)\n",
    "    print(model)\n",
    "\n",
    "    model=\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=optimizer_config[\"lr\"], betas=(optimizer_config[\"beta1\"], optimizer_config[\"beta2\"]))\n",
    "    # optimizer = Nadam(model.parameters(),\n",
    "    #                        lr=optimizer_config[\"lr\"], betas=(optimizer_config[\"beta1\"], optimizer_config[\"beta2\"]))\n",
    "\n",
    "    # optimizer = optim.RMSprop(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    warm_up = optimizer_config.get(\"warm_up\", False)\n",
    "\n",
    "    scheduler = WarmupScheduler(optimizer=optimizer,\n",
    "                                steps=optimizer_config[\"warm_up_steps\"] if warm_up else 0,\n",
    "                                multiplier=optimizer_config[\"warm_up_factor\"] if warm_up else 1)\n",
    "\n",
    "    decay_done = False\n",
    "    max_acc = 0\n",
    "\n",
    "    with config_path.open(\"w\") as fp:\n",
    "        json.dump(config, fp, indent=4)\n",
    "    if eval_test:\n",
    "        print(f\"testing ... load {model_path.absolute()}\")\n",
    "        model.load_state_dict(torch.load(model_path.absolute()))\n",
    "        # Evaluation on test data\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            total_test_samples = 0\n",
    "            single_task_acc = [0] * len(test_data_loaders)\n",
    "            for k, te in test_data_loaders.items():\n",
    "                test_data_loader = te\n",
    "                task_acc = 0\n",
    "                single_task_samples = 0\n",
    "                for story, story_length, query, answer in tqdm(test_data_loader):\n",
    "                    logits = model({\"story\":story.to(device), \"query\":query.to(device)})\n",
    "                    answer = answer.to(device)\n",
    "                    correct_batch = (torch.argmax(logits, dim=-1) == answer).sum()\n",
    "                    correct += correct_batch.item()\n",
    "                    task_acc += correct_batch.item()\n",
    "                    loss = loss_fn(logits, answer)\n",
    "                    test_loss += loss.sum().item()\n",
    "                    total_test_samples+=story.shape[0]\n",
    "                    single_task_samples+=story.shape[0]\n",
    "                print(f\"validate acc task {k}: {task_acc/single_task_samples}\")\n",
    "                single_task_acc[k - 1] = task_acc/single_task_samples\n",
    "            test_acc = correct / total_test_samples\n",
    "            test_loss = test_loss / total_test_samples\n",
    "        print(f\"Test accuracy: {test_acc:.3f}, loss: {test_loss:.3f}\")\n",
    "        print(f\"test avg: {np.mean(single_task_acc)}\")\n",
    "        raise True\n",
    "    \n",
    "    # dataset analyze\n",
    "    print(num_train_batches)\n",
    "    for _ in tqdm(range(num_train_batches)):\n",
    "        loader_i = random.randint(0,len(train_data_loaders)-1)+1\n",
    "        try:\n",
    "            story, story_length, query, answer = next(train_data_loaders[loader_i][0])\n",
    "        except StopIteration:\n",
    "            train_data_loaders[loader_i][0] = iter(train_data_loaders[loader_i][1])\n",
    "            story, story_length, query, answer = next(train_data_loaders[loader_i][0])\n",
    "        #print(type(story)) #torch.Tensor\n",
    "        #print(\"story size:\",story.size())#(10,sentence, words)\n",
    "        #print(\"query size:\",query.size())#(10,words)\n",
    "        #print(\"answer size:\",answer.size())#(10)\n",
    "        #break\n",
    "\n",
    "    \n",
    "    for i in range(trainer_config[\"epochs\"]):\n",
    "        logging.info(f\"##### EPOCH: {i} #####\")\n",
    "        scheduler.step()\n",
    "        # Train\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "        for _ in tqdm(range(num_train_batches)):\n",
    "            loader_i = random.randint(0,len(train_data_loaders)-1)+1\n",
    "            try:\n",
    "                story, story_length, query, answer = next(train_data_loaders[loader_i][0])\n",
    "            except StopIteration:\n",
    "                train_data_loaders[loader_i][0] = iter(train_data_loaders[loader_i][1])\n",
    "                story, story_length, query, answer = next(train_data_loaders[loader_i][0])\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(story.to(device), query.to(device))\n",
    "            answer = answer.to(device)\n",
    "            correct_batch = (torch.argmax(logits, dim=-1) == answer).sum()\n",
    "            correct += correct_batch.item()\n",
    "\n",
    "            loss = loss_fn(logits, answer)\n",
    "            train_loss += loss.sum().item()\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), optimizer_config[\"max_gradient_norm\"])\n",
    "            # nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc = correct / (num_train_batches*trainer_config[\"batch_size\"])\n",
    "        train_loss = train_loss / (num_train_batches*trainer_config[\"batch_size\"])\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            total_valid_samples = 0\n",
    "            for  k, va in valid_data_loaders.items():\n",
    "                valid_data_loader = va\n",
    "                task_acc = 0\n",
    "                single_valid_samples = 0\n",
    "                for story, story_length, query, answer in valid_data_loader:\n",
    "                    logits = model(story.to(device), query.to(device))\n",
    "                    answer = answer.to(device)\n",
    "                    correct_batch = (torch.argmax(logits, dim=-1) == answer).sum()\n",
    "                    correct += correct_batch.item()\n",
    "                    loss = loss_fn(logits, answer)\n",
    "                    valid_loss += loss.sum().item()\n",
    "                    task_acc += correct_batch.item()\n",
    "                    total_valid_samples+= story.shape[0]\n",
    "                    single_valid_samples+= story.shape[0]\n",
    "                print(f\"validate acc task {k}: {task_acc/single_valid_samples}\")\n",
    "            valid_acc = correct / total_valid_samples\n",
    "            valid_loss = valid_loss / total_valid_samples\n",
    "            if valid_acc>max_acc:\n",
    "                print(f\"saved model...{model_path}\")\n",
    "                torch.save(model.state_dict(), model_path.absolute())\n",
    "                max_acc = valid_acc\n",
    "\n",
    "        writer.add_scalars(\"accuracy\", {\"train\": train_acc,\n",
    "                                        \"validation\": valid_acc}, i)\n",
    "        writer.add_scalars(\"loss\", {\"train\": train_loss,\n",
    "                                    \"validation\": valid_loss}, i)\n",
    "\n",
    "        logging.info(f\"\\nTrain accuracy: {train_acc:.3f}, loss: {train_loss:.3f}\"\n",
    "                     f\"\\nValid accuracy: {valid_acc:.3f}, loss: {valid_loss:.3f}\")\n",
    "        if optimizer_config.get(\"decay\", False) and valid_loss < optimizer_config[\"decay_thr\"] and not decay_done:\n",
    "            scheduler.decay_lr(optimizer_config[\"decay_factor\"])\n",
    "            decay_done = True\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa1_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa1_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa1_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa2_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa2_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa2_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa3_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa3_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa3_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa4_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa4_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa4_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa5_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa5_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa5_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa6_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa6_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa6_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa7_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa7_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa7_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa8_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa8_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa8_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa9_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa9_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa9_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa10_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa10_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa10_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa11_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa11_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa11_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa12_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa12_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa12_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa13_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa13_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa13_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa14_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa14_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa14_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa15_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa15_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa15_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa16_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa16_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa16_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa17_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa17_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa17_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa18_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa18_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa18_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa19_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa19_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa19_test.pik\n",
      "read train ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa20_train.pik\n",
      "read valid ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa20_valid.pik\n",
      "read test ...\n",
      "accessing cache_path:  ./babi/data/en-valid-10k-pik/qa20_test.pik\n",
      "total train data: 180010\n",
      "total valid data: 20033\n",
      "total test data: 20020\n",
      "voca size 179\n",
      "QAmodel(\n",
      "  (input_module): InputModule(\n",
      "    (word_embed): Embedding(179, 64)\n",
      "  )\n",
      "  (update_module): STM(\n",
      "    (qkv_projector): ModuleList(\n",
      "      (0): Linear(in_features=90, out_features=60, bias=True)\n",
      "    )\n",
      "    (qkv_layernorm): ModuleList(\n",
      "      (0): LayerNorm((90, 60), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (input_projector): MLP(\n",
      "      (fc1): Linear(in_features=64, out_features=80, bias=True)\n",
      "      (fc2h): ModuleList(\n",
      "        (0): Linear(in_features=80, out_features=80, bias=True)\n",
      "      )\n",
      "      (fc3): Linear(in_features=80, out_features=90, bias=True)\n",
      "    )\n",
      "    (input_projector2): MLP(\n",
      "      (fc1): Linear(in_features=64, out_features=80, bias=True)\n",
      "      (fc2h): ModuleList(\n",
      "        (0): Linear(in_features=80, out_features=80, bias=True)\n",
      "      )\n",
      "      (fc3): Linear(in_features=80, out_features=90, bias=True)\n",
      "    )\n",
      "    (input_projector3): MLP(\n",
      "      (fc1): Linear(in_features=64, out_features=80, bias=True)\n",
      "      (fc2h): ModuleList(\n",
      "        (0): Linear(in_features=80, out_features=80, bias=True)\n",
      "      )\n",
      "      (fc3): Linear(in_features=80, out_features=20, bias=True)\n",
      "    )\n",
      "    (input_gate_projector): Linear(in_features=90, out_features=180, bias=True)\n",
      "    (memory_gate_projector): Linear(in_features=90, out_features=180, bias=True)\n",
      "    (rel_projector): Linear(in_features=8100, out_features=96, bias=True)\n",
      "    (rel_projector2): Linear(in_features=1800, out_features=90, bias=True)\n",
      "    (rel_projector3): Linear(in_features=1920, out_features=64, bias=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (out): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (infer_module): InferenceModule(\n",
      "    (e): ModuleList(\n",
      "      (0): MLP()\n",
      "      (1): MLP()\n",
      "    )\n",
      "    (r): ModuleList(\n",
      "      (0): MLP()\n",
      "      (1): MLP()\n",
      "      (2): MLP()\n",
      "    )\n",
      "    (l1): OptionalLayer(\n",
      "      (layer): LayerNorm()\n",
      "    )\n",
      "    (l2): OptionalLayer(\n",
      "      (layer): LayerNorm()\n",
      "    )\n",
      "    (l3): OptionalLayer(\n",
      "      (layer): LayerNorm()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "18001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "story size: torch.Size([10, 10, 9])\n",
      "query size: torch.Size([10, 9])\n",
      "answer size: torch.Size([10])\n",
      "tensor([[[ 3, 94, 10,  7, 17,  6, 20,  5,  0],\n",
      "         [ 9, 94, 10, 18, 17,  6,  4,  5,  0],\n",
      "         [ 3, 94, 10, 11, 14, 17,  6, 12,  5],\n",
      "         [10, 94,  3, 11, 14, 17,  6, 16,  5],\n",
      "         [ 1, 94,  9, 11, 14, 17,  6, 20,  5],\n",
      "         [ 1, 94,  9, 18, 17,  6,  4,  5,  0],\n",
      "         [10, 94,  9,  7, 17,  6,  8,  5,  0],\n",
      "         [ 9, 94, 10, 15, 17,  6, 13,  5,  0],\n",
      "         [ 1, 94, 10, 18, 17,  6, 12,  5,  0],\n",
      "         [10, 94,  9, 15, 17,  6, 20,  5,  0]],\n",
      "\n",
      "        [[ 1, 94, 10,  7, 17,  6,  4,  5,  0],\n",
      "         [ 9, 94, 10,  7, 17,  6, 12,  5,  0],\n",
      "         [10, 94,  3, 18, 17,  6, 20,  5,  0],\n",
      "         [ 9, 94,  1,  7, 17,  6,  8,  5,  0],\n",
      "         [ 1, 94,  9,  7, 17,  6, 12,  5,  0],\n",
      "         [ 1, 94,  3, 18, 17,  6, 13,  5,  0],\n",
      "         [ 3, 94,  1, 11, 14, 17,  6, 16,  5],\n",
      "         [ 3, 94,  1,  7, 17,  6,  8,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 1, 94,  9, 11, 14, 17,  6,  4,  5],\n",
      "         [ 3, 94,  9, 15, 17,  6, 13,  5,  0],\n",
      "         [ 3, 94, 10, 11, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94,  3, 15, 17,  6,  8,  5,  0],\n",
      "         [ 9, 94,  3, 15, 17,  6, 20,  5,  0],\n",
      "         [10, 94,  1, 15, 17,  6,  8,  5,  0],\n",
      "         [10, 94,  9, 18, 17,  6,  4,  5,  0],\n",
      "         [ 1, 94,  3, 11, 17,  6,  4,  5,  0],\n",
      "         [ 3, 94,  1, 11, 14, 17,  6,  8,  5],\n",
      "         [ 9, 94, 10, 18, 17,  6,  8,  5,  0]],\n",
      "\n",
      "        [[ 3, 94,  9, 11, 14, 17,  6, 12,  5],\n",
      "         [ 9, 94,  3, 18, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94, 10, 11, 17,  6, 13,  5,  0],\n",
      "         [10, 94,  9,  7, 17,  6, 20,  5,  0],\n",
      "         [ 1, 94,  9, 11, 17,  6, 16,  5,  0],\n",
      "         [ 1, 94, 10,  7, 17,  6,  8,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 1, 94, 10, 15, 17,  6, 13,  5,  0],\n",
      "         [ 3, 94,  1,  7, 17,  6,  8,  5,  0],\n",
      "         [ 9, 94, 10, 11, 17,  6, 12,  5,  0],\n",
      "         [ 9, 94, 10,  7, 17,  6,  4,  5,  0],\n",
      "         [ 3, 94,  1, 11, 17,  6, 13,  5,  0],\n",
      "         [ 9, 94,  1,  7, 17,  6, 12,  5,  0],\n",
      "         [ 3, 94,  1, 11, 17,  6, 16,  5,  0],\n",
      "         [ 3, 94,  1,  7, 17,  6, 12,  5,  0],\n",
      "         [ 9, 94,  1, 15, 17,  6, 20,  5,  0],\n",
      "         [ 1, 94,  9, 11, 17,  6,  8,  5,  0]],\n",
      "\n",
      "        [[ 9, 94,  3,  7, 17,  6, 13,  5,  0],\n",
      "         [10, 94,  9,  7, 17,  6, 16,  5,  0],\n",
      "         [ 1, 94,  9, 11, 17,  6, 12,  5,  0],\n",
      "         [ 9, 94, 10, 18, 17,  6,  8,  5,  0],\n",
      "         [10, 94,  1,  7, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94,  1, 18, 17,  6, 20,  5,  0],\n",
      "         [ 3, 94, 10, 15, 17,  6, 20,  5,  0],\n",
      "         [ 3, 94,  9,  7, 17,  6, 13,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 9, 94,  1, 11, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94,  3, 11, 14, 17,  6, 20,  5],\n",
      "         [ 3, 94,  1, 11, 14, 17,  6,  8,  5],\n",
      "         [ 9, 94,  1, 15, 17,  6,  4,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[10, 94,  9, 15, 17,  6, 13,  5,  0],\n",
      "         [10, 94,  9, 18, 17,  6,  4,  5,  0],\n",
      "         [ 1, 94, 10, 15, 17,  6, 12,  5,  0],\n",
      "         [ 3, 94, 10, 15, 17,  6,  8,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 3, 94,  9, 11, 14, 17,  6, 12,  5],\n",
      "         [ 9, 94,  1, 11, 14, 17,  6,  8,  5],\n",
      "         [ 3, 94, 10, 18, 17,  6, 16,  5,  0],\n",
      "         [ 3, 94, 10, 18, 17,  6, 13,  5,  0],\n",
      "         [ 1, 94,  3,  7, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94,  3, 11, 14, 17,  6,  4,  5],\n",
      "         [ 3, 94,  1, 15, 17,  6, 12,  5,  0],\n",
      "         [ 3, 94,  1, 11, 17,  6, 20,  5,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "\n",
      "        [[ 9, 94, 10,  7, 17,  6, 16,  5,  0],\n",
      "         [ 9, 94,  1,  7, 17,  6, 13,  5,  0],\n",
      "         [ 9, 94,  1, 18, 17,  6, 20,  5,  0],\n",
      "         [ 9, 94,  1,  7, 17,  6, 13,  5,  0],\n",
      "         [ 9, 94, 10, 15, 17,  6, 12,  5,  0],\n",
      "         [ 1, 94,  3, 18, 17,  6, 12,  5,  0],\n",
      "         [ 1, 94,  3,  7, 17,  6, 13,  5,  0],\n",
      "         [ 1, 94,  3, 18, 17,  6, 12,  5,  0],\n",
      "         [ 1, 94,  9, 15, 17,  6,  8,  5,  0],\n",
      "         [ 9, 94,  1, 15, 17,  6,  4,  5,  0]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser(description=\"Training script.\")\n",
    "    parser.add_argument(\"--config-file\", type=str, metavar='PATH', default=\"./babi/configs/config_all.json\",\n",
    "                        help=\"Path to the model config file\")\n",
    "    parser.add_argument(\"--serialization-path\", type=str, metavar='PATH', default=\"./saved_models/\",\n",
    "                        help=\"Serialization directory path\")\n",
    "    parser.add_argument(\"--eval-test\", default=False, action='store_true',\n",
    "                        help=\"Whether to eval model on test dataset after training (default: False)\")\n",
    "    parser.add_argument(\"--logging-level\", type=str, metavar='LEVEL', default=20, choices=range(10, 51, 10),\n",
    "                        help=\"Logging level (default: 20)\")\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    logging.basicConfig(level=args.logging_level)\n",
    "\n",
    "    with open(args.config_file, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    train(config, args.serialization_path, args.eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/my_modified_ntm', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/work/my_modified_ntm/bgavDNC_src']\n",
      "['/work/my_modified_ntm', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/work/my_modified_ntm/bgavDNC_src', '/work/my_modified_ntm/bgavDNC_src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"/work/my_modified_ntm/bgavDNC_src\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_implementations.bAbI.bAbI import *\n",
    "from utils import project_path, init_wrapper\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded task\n"
     ]
    }
   ],
   "source": [
    "task = bAbITask(os.path.join(\"tasks_1-20_v1-2\", \"en-10k\"))\n",
    "print(\"Loaded task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded task\n"
     ]
    }
   ],
   "source": [
    "task = bAbITask(os.path.join(\"tasks_1-20_v1-2\", \"en-10k\"))\n",
    "print(\"Loaded task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "{0: '-', 1: '*', 2: 'mary', 3: 'is', 4: 'no', 5: 'longer', 6: 'in', 7: 'the', 8: 'bedroom', 9: '.', 10: 'daniel', 11: 'moved', 12: 'to', 13: 'hallway', 14: '?', 15: 'sandra', 16: 'bathroom', 17: 'not', 18: 'office', 19: 'went', 20: 'john', 21: 'kitchen', 22: 'travelled', 23: 'back', 24: 'garden', 25: 'yes', 26: 'journeyed', 27: 'sumit', 28: 'tired', 29: 'where', 30: 'will', 31: 'go', 32: 'why', 33: 'did', 34: 'grabbed', 35: 'pajamas', 36: 'there', 37: 'get', 38: 'yann', 39: 'bored', 40: 'jason', 41: 'thirsty', 42: 'got', 43: 'football', 44: 'antoine', 45: 'milk', 46: 'hungry', 47: 'took', 48: 'apple', 49: 'picked', 50: 'up', 51: 'bill', 52: 'gave', 53: 'fred', 54: 'what', 55: 'give', 56: 'handed', 57: 'jeff', 58: 'who', 59: 'received', 60: 'left', 61: 'passed', 62: 'put', 63: 'down', 64: 'dropped', 65: 'discarded', 66: 'carrying', 67: 'nothing', 68: 'how', 69: 'many', 70: 'objects', 71: 'one', 72: 'none', 73: 'two', 74: 'three', 75: 'cinema', 76: 'yesterday', 77: 'julie', 78: 'school', 79: 'this', 80: 'morning', 81: 'park', 82: 'was', 83: 'before', 84: 'afternoon', 85: 'evening', 86: 'either', 87: 'or', 88: 'maybe', 89: 'after', 90: 'that', 91: 'she', 92: 'afterwards', 93: 'he', 94: 'following', 95: 'then', 96: 'wolves', 97: 'are', 98: 'afraid', 99: 'of', 100: 'mice', 101: 'sheep', 102: 'winona', 103: 'a', 104: 'cats', 105: 'jessica', 106: 'mouse', 107: 'emily', 108: 'cat', 109: 'gertrude', 110: 'wolf', 111: 'north', 112: 'south', 113: 'west', 114: 'east', 115: 'suitcase', 116: 'fits', 117: 'inside', 118: 'box', 119: 'chocolate', 120: 'container', 121: 'bigger', 122: 'than', 123: 'chocolates', 124: 'chest', 125: 'does', 126: 'fit', 127: 'and', 128: 'they', 129: 'triangle', 130: 'above', 131: 'pink', 132: 'rectangle', 133: 'blue', 134: 'square', 135: 'right', 136: 'below', 137: 'red', 138: 'sphere', 139: 'yellow', 140: 'lily', 141: 'frog', 142: 'bernhard', 143: 'green', 144: 'brian', 145: 'lion', 146: 'white', 147: 'julius', 148: 'swan', 149: 'greg', 150: 'color', 151: 'rhino', 152: 'gray', 153: 'do', 154: 'you', 155: 'from', 156: 's', 157: 'e', 158: 'n', 159: 'w'}\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "print(task.vector_size)\n",
    "print(task.ind_to_word)\n",
    "print(task.ind_to_word[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 160]\n",
      "[None, None, 1]\n"
     ]
    }
   ],
   "source": [
    "print(task.x_shape)\n",
    "print(task.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa9_simple-negation_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa20_agents-motivations_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa14_time-reasoning_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa11_basic-coreference_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa13_compound-coreference_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa16_basic-induction_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa15_basic-deduction_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa7_counting_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa8_lists-sets_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa12_conjunction_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa19_path-finding_train.txt', '/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa18_size-reasoning_train.txt'])\n"
     ]
    }
   ],
   "source": [
    "print(task.x_train_stories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46505/1198921949.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  xarr=np.array(xlis)\n"
     ]
    }
   ],
   "source": [
    "xlis=task.y_test_stories[\"/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa8_lists-sets_test.txt\"]\n",
    "print(type(xlis))\n",
    "xarr=np.array(xlis)\n",
    "print(xarr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'got', 'the', 'milk', 'there', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'what', 'is', 'mary', 'carrying', '?', '-', 'john', 'picked', 'up', 'the', 'football', 'there', '.', 'john', 'journeyed', 'to', 'the', 'bathroom', '.', 'what', 'is', 'john', 'carrying', '?', '-', 'john', 'went', 'to', 'the', 'garden', '.', 'daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'what', 'is', 'john', 'carrying', '?', '-', 'john', 'went', 'back', 'to', 'the', 'bathroom', '.', 'mary', 'went', 'to', 'the', 'office', '.', 'sandra', 'went', 'back', 'to', 'the', 'kitchen', '.', 'mary', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'travelled', 'to', 'the', 'bedroom', '.', 'john', 'picked', 'up', 'the', 'apple', 'there', '.', 'what', 'is', 'john', 'carrying', '?', '-', '-', 'sandra', 'travelled', 'to', 'the', 'bedroom', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'what', 'is', 'john', 'carrying', '?', '-', '-']\n",
      "['mary', 'got', 'the', 'milk', 'there', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'what', 'is', 'mary', 'carrying', '?', 'milk', 'john', 'picked', 'up', 'the', 'football', 'there', '.', 'john', 'journeyed', 'to', 'the', 'bathroom', '.', 'what', 'is', 'john', 'carrying', '?', 'football', 'john', 'went', 'to', 'the', 'garden', '.', 'daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'what', 'is', 'john', 'carrying', '?', 'football', 'john', 'went', 'back', 'to', 'the', 'bathroom', '.', 'mary', 'went', 'to', 'the', 'office', '.', 'sandra', 'went', 'back', 'to', 'the', 'kitchen', '.', 'mary', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'travelled', 'to', 'the', 'bedroom', '.', 'john', 'picked', 'up', 'the', 'apple', 'there', '.', 'what', 'is', 'john', 'carrying', '?', 'football', 'apple', 'sandra', 'travelled', 'to', 'the', 'bedroom', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'what', 'is', 'john', 'carrying', '?', 'football', 'apple']\n"
     ]
    }
   ],
   "source": [
    "def idlist_to_word(lis):\n",
    "    return [task.ind_to_word[elm] for elm in lis]\n",
    "print(idlist_to_word( task.x_test_stories[\"/work/my_modified_ntm/bgavDNC_src/task_implementations/bAbI/tasks_1-20_v1-2/en-10k/qa8_lists-sets_test.txt\"][0] ))\n",
    "print(idlist_to_word( xlis[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 179]\n",
      "(2, 179, 1)\n",
      "160 (2, 179, 160) (2, 179, 160)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "cost_value=999\n",
    "batch_size=2\n",
    "data_batch, seqlen, m = task.generate_data(cost=cost_value, batch_size=batch_size, train=True)\n",
    "print(seqlen)\n",
    "print(m.shape)\n",
    "print(task.vector_size,data_batch[0].shape,data_batch[1].shape)\n",
    "print(type(data_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n",
      "[['the' 'bedroom' 'is' 'east' 'of' 'the' 'kitchen' '.' 'the' 'office'\n",
      "  'is' 'north' 'of' 'the' 'kitchen' '.' 'the' 'bathroom' 'is' 'north'\n",
      "  'of' 'the' 'garden' '.' 'the' 'kitchen' 'is' 'east' 'of' 'the' 'garden'\n",
      "  '.' 'the' 'hallway' 'is' 'west' 'of' 'the' 'bathroom' '.' 'how' 'do'\n",
      "  'you' 'go' 'from' 'the' 'kitchen' 'to' 'the' 'bathroom' '?' '-' '-' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*']\n",
      " ['jeff' 'moved' 'to' 'the' 'hallway' '.' 'bill' 'went' 'back' 'to' 'the'\n",
      "  'kitchen' '.' 'jeff' 'travelled' 'to' 'the' 'garden' '.' 'jeff' 'went'\n",
      "  'back' 'to' 'the' 'hallway' '.' 'bill' 'journeyed' 'to' 'the' 'garden'\n",
      "  '.' 'mary' 'moved' 'to' 'the' 'office' '.' 'mary' 'got' 'the' 'apple'\n",
      "  'there' '.' 'jeff' 'took' 'the' 'milk' 'there' '.' 'jeff' 'went' 'to'\n",
      "  'the' 'garden' '.' 'jeff' 'moved' 'to' 'the' 'bathroom' '.' 'bill'\n",
      "  'went' 'back' 'to' 'the' 'hallway' '.' 'jeff' 'moved' 'to' 'the'\n",
      "  'garden' '.' 'jeff' 'passed' 'the' 'milk' 'to' 'fred' '.' 'fred'\n",
      "  'handed' 'the' 'milk' 'to' 'jeff' '.' 'who' 'received' 'the' 'milk' '?'\n",
      "  '-' 'jeff' 'passed' 'the' 'milk' 'to' 'fred' '.' 'fred' 'passed' 'the'\n",
      "  'milk' 'to' 'jeff' '.' 'what' 'did' 'fred' 'give' 'to' 'jeff' '?' '-'\n",
      "  'jeff' 'gave' 'the' 'milk' 'to' 'fred' '.' 'fred' 'handed' 'the' 'milk'\n",
      "  'to' 'jeff' '.' 'who' 'gave' 'the' 'milk' '?' '-' 'jeff' 'passed' 'the'\n",
      "  'milk' 'to' 'fred' '.' 'fred' 'passed' 'the' 'milk' 'to' 'jeff' '.'\n",
      "  'who' 'gave' 'the' 'milk' '?' '-' 'jeff' 'handed' 'the' 'milk' 'to'\n",
      "  'fred' '.' 'fred' 'passed' 'the' 'milk' 'to' 'jeff' '.' 'who' 'gave'\n",
      "  'the' 'milk' 'to' 'jeff' '?' '-']]\n",
      "print mask [:,:,0] :\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "print y:\n",
      "[[  7   8   3 114  99   7  21   9   7  18   3 111  99   7  21   9   7  16\n",
      "    3 111  99   7  24   9   7  21   3 114  99   7  24   9   7  13   3 113\n",
      "   99   7  16   9  68 153 154  31 155   7  21  12   7  16  14 159 158   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
      " [ 57  11  12   7  13   9  51  19  23  12   7  21   9  57  22  12   7  24\n",
      "    9  57  19  23  12   7  13   9  51  26  12   7  24   9   2  11  12   7\n",
      "   18   9   2  42   7  48  36   9  57  47   7  45  36   9  57  19  12   7\n",
      "   24   9  57  11  12   7  16   9  51  19  23  12   7  13   9  57  11  12\n",
      "    7  24   9  57  61   7  45  12  53   9  53  56   7  45  12  57   9  58\n",
      "   59   7  45  14  57  57  61   7  45  12  53   9  53  61   7  45  12  57\n",
      "    9  54  33  53  55  12  57  14  45  57  52   7  45  12  53   9  53  56\n",
      "    7  45  12  57   9  58  52   7  45  14  53  57  61   7  45  12  53   9\n",
      "   53  61   7  45  12  57   9  58  52   7  45  14  53  57  56   7  45  12\n",
      "   53   9  53  61   7  45  12  57   9  58  52   7  45  12  57  14  53]]\n",
      "[['the' 'bedroom' 'is' 'east' 'of' 'the' 'kitchen' '.' 'the' 'office'\n",
      "  'is' 'north' 'of' 'the' 'kitchen' '.' 'the' 'bathroom' 'is' 'north'\n",
      "  'of' 'the' 'garden' '.' 'the' 'kitchen' 'is' 'east' 'of' 'the' 'garden'\n",
      "  '.' 'the' 'hallway' 'is' 'west' 'of' 'the' 'bathroom' '.' 'how' 'do'\n",
      "  'you' 'go' 'from' 'the' 'kitchen' 'to' 'the' 'bathroom' '?' 'w' 'n' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*'\n",
      "  '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*' '*']\n",
      " ['jeff' 'moved' 'to' 'the' 'hallway' '.' 'bill' 'went' 'back' 'to' 'the'\n",
      "  'kitchen' '.' 'jeff' 'travelled' 'to' 'the' 'garden' '.' 'jeff' 'went'\n",
      "  'back' 'to' 'the' 'hallway' '.' 'bill' 'journeyed' 'to' 'the' 'garden'\n",
      "  '.' 'mary' 'moved' 'to' 'the' 'office' '.' 'mary' 'got' 'the' 'apple'\n",
      "  'there' '.' 'jeff' 'took' 'the' 'milk' 'there' '.' 'jeff' 'went' 'to'\n",
      "  'the' 'garden' '.' 'jeff' 'moved' 'to' 'the' 'bathroom' '.' 'bill'\n",
      "  'went' 'back' 'to' 'the' 'hallway' '.' 'jeff' 'moved' 'to' 'the'\n",
      "  'garden' '.' 'jeff' 'passed' 'the' 'milk' 'to' 'fred' '.' 'fred'\n",
      "  'handed' 'the' 'milk' 'to' 'jeff' '.' 'who' 'received' 'the' 'milk' '?'\n",
      "  'jeff' 'jeff' 'passed' 'the' 'milk' 'to' 'fred' '.' 'fred' 'passed'\n",
      "  'the' 'milk' 'to' 'jeff' '.' 'what' 'did' 'fred' 'give' 'to' 'jeff' '?'\n",
      "  'milk' 'jeff' 'gave' 'the' 'milk' 'to' 'fred' '.' 'fred' 'handed' 'the'\n",
      "  'milk' 'to' 'jeff' '.' 'who' 'gave' 'the' 'milk' '?' 'fred' 'jeff'\n",
      "  'passed' 'the' 'milk' 'to' 'fred' '.' 'fred' 'passed' 'the' 'milk' 'to'\n",
      "  'jeff' '.' 'who' 'gave' 'the' 'milk' '?' 'fred' 'jeff' 'handed' 'the'\n",
      "  'milk' 'to' 'fred' '.' 'fred' 'passed' 'the' 'milk' 'to' 'jeff' '.'\n",
      "  'who' 'gave' 'the' 'milk' 'to' 'jeff' '?' 'fred']]\n",
      "input shape  torch.Size([2, 179, 160])  mask shape torch.Size([2, 179, 1])\n",
      "masked_input: torch.Size([7, 160])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([159, 158,  57,  45,  53,  53,  53], device='cuda:0')\n",
      "masked y to word:\n",
      "['w', 'n', 'jeff', 'milk', 'fred', 'fred', 'fred']\n"
     ]
    }
   ],
   "source": [
    "#masked_select test \n",
    "import torch\n",
    "def tensor2np(tsr):\n",
    "    return np.array(tsr.clone().detach().cpu())\n",
    "def idlist2word_batch2D(idlist_batch):\n",
    "    listOFstrlist =[idlist_to_word(seq) for seq in idlist_batch]\n",
    "    return np.array(listOFstrlist)\n",
    "def OH2word_batch3D(oh_list):\n",
    "    idlist_batch=np.argmax(oh_list, axis=2)\n",
    "    return idlist2word_batch2D(idlist_batch)    \n",
    "\n",
    "#numpy viz\n",
    "#x,mask\n",
    "print(type(OH2word_batch3D(data_batch[0])))\n",
    "print(data_batch[0])\n",
    "print(OH2word_batch3D(data_batch[0]))\n",
    "print(\"print mask [:,:,0] :\")\n",
    "print(m[:,:,0])\n",
    "#y\n",
    "print(\"print y:\")\n",
    "target_np=np.argmax(data_batch[1],axis=2)\n",
    "print(target_np)\n",
    "print(idlist2word_batch2D(target_np))\n",
    "\n",
    "#tensorize x and mask\n",
    "input =torch.tensor(data_batch[0],dtype=torch.float).to(0)\n",
    "mask=torch.tensor(m==1).to(0,dtype=torch.bool)\n",
    "\n",
    "print(\"input shape \",input.shape,\" mask shape\",mask.shape)\n",
    "masked_input=torch.masked_select(input,mask).view(-1,160)\n",
    "print(\"masked_input:\",masked_input.shape)\n",
    "for e in masked_input:\n",
    "    print(e)\n",
    "\n",
    "#tesor y and mask\n",
    "target=torch.tensor(target_np).to(0)\n",
    "masked_target=torch.masked_select(target,mask[:,:,0])\n",
    "print(masked_target)\n",
    "print(\"masked y to word:\")\n",
    "print(idlist_to_word(tensor2np(masked_target)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "<class 'tuple'>\n",
      "2\n",
      "[[20, 22, 12, 7, 13, 9, 2, 26, 12, 7, 16, 9, 29, 3, 20, 14, 0, 10, 19, 23, 12, 7, 16, 9, 20, 11, 12, 7, 8, 9, 29, 3, 2, 14, 0, 20, 19, 12, 7, 13, 9, 15, 26, 12, 7, 21, 9, 29, 3, 15, 14, 0, 15, 22, 12, 7, 13, 9, 20, 19, 12, 7, 24, 9, 29, 3, 15, 14, 0, 15, 19, 23, 12, 7, 16, 9, 15, 11, 12, 7, 21, 9, 29, 3, 15, 14, 0], [15, 22, 12, 7, 21, 9, 15, 22, 12, 7, 13, 9, 29, 3, 15, 14, 0, 2, 19, 12, 7, 16, 9, 15, 11, 12, 7, 24, 9, 29, 3, 15, 14, 0, 15, 22, 12, 7, 18, 9, 10, 26, 12, 7, 13, 9, 29, 3, 10, 14, 0, 10, 26, 12, 7, 18, 9, 20, 11, 12, 7, 13, 9, 29, 3, 15, 14, 0, 20, 22, 12, 7, 16, 9, 20, 26, 12, 7, 18, 9, 29, 3, 10, 14, 0]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "num_tasks = len(task.x_test_stories)\n",
    "print(num_tasks)\n",
    "for (inp,output) in zip(task.x_test_stories.items(), task.y_test_stories.items()):\n",
    "    #inp, out　どちらも(key,value) のタプルっぽい。key=storyごとにforで回す　forのひとつひとつは1key= 1task_type\n",
    "    print(type(inp))\n",
    "    print(len(inp))\n",
    "    print(inp[1][:2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4576, 0.4576, 0.0619, 0.0228]], dtype=torch.float64)\n",
      "tensor([[0.3012, 0.3012, 0.2027, 0.1950]], dtype=torch.float64)\n",
      "tensor(0.7817, dtype=torch.float64) tensor(1.2001, dtype=torch.float64)\n",
      "tensor([[0.4676, 0.4676, 0.0719, 0.0328]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#softmax, loss test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "intlis=torch.tensor([[3,3,1,0]],dtype=float)\n",
    "soft1=nn.functional.softmax(intlis,dim=1)\n",
    "soft2=nn.functional.softmax(soft1,dim=1)\n",
    "print(soft1)\n",
    "print(soft2)\n",
    "label=torch.tensor([1])\n",
    "cros=nn.CrossEntropyLoss()\n",
    "loss1=cros(intlis,label)\n",
    "loss2=cros(soft1,label)\n",
    "print(loss1,loss2)\n",
    "print(soft1+1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6265e+00, 1.1144e-01, 1.3619e-04], dtype=torch.float64)\n",
      "tensor(0.5794, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testints=torch.tensor([[1,2,1,0],\n",
    "                        [1,4,0,1],\n",
    "                        [0,0,10,0]],\n",
    "                        dtype=float)\n",
    "labels=torch.tensor([0,1,2]) # bad, good, great\n",
    "cross_nored=nn.CrossEntropyLoss(reduction='none')\n",
    "cros=nn.CrossEntropyLoss()\n",
    "print(cross_nored(testints,labels))\n",
    "print(cros(testints, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycross_OH_3Dto3D(input, target, reduction='mean'):\n",
    "    if reduction!=\"mean\": raise ValueError('reduction is not implemented yet')\n",
    "    if (input.dim()!=3) or (target.dim()!=3): raise ValueError('invalid dim')\n",
    "    ls_input=nn.functional.log_softmax(input, 2)\n",
    "    nll=-1*torch.einsum( 'blf,blf->bl',ls_input,target) \n",
    "    #nll=torch.mean(nll,dim=1)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "torch.float64 torch.float64\n",
      "tensor([[1.6265e+00, 1.1144e-01, 1.3619e-04]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "testints3=torch.tensor([[[1,2,1,0],\n",
    "                        [1,4,0,1],\n",
    "                        [0,0,10,0]]],\n",
    "                        dtype=float)\n",
    "labels3=torch.tensor([[0,1,2]]) # bad, good, great\n",
    "labels3_onehot=nn.functional.one_hot(labels3,num_classes=4).to(torch.float64)\n",
    "print(testints3.dim(),labels3_onehot.dim())\n",
    "print(testints3.dtype,labels3_onehot.dtype)\n",
    "print(mycross_OH_3Dto3D(testints3,labels3_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_masked_mean(loss,mask):\n",
    "    masked = loss * mask[:, :, 0]\n",
    "    mask_batchs =torch.sum(mask[:,:,0],dim=1)\n",
    "    return torch.mean(torch.sum(masked,dim=1)/ mask_batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 4]) torch.Size([2, 7]) torch.Size([2, 7, 4]) torch.Size([2, 7, 1])\n",
      "tensor([[1.6265e+00, 1.1144e-01, 1.3619e-04, 1.0000e+01, 1.0000e+01, 1.0000e+01,\n",
      "         1.0000e+01],\n",
      "        [1.6265e+00, 1.1144e-01, 1.3619e-04, 1.0000e+01, 1.0000e+01, 1.0000e+01,\n",
      "         1.0000e+01]], dtype=torch.float64)\n",
      "tensor([[1.6265e+00, 1.1144e-01, 1.3619e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.1144e-01, 1.3619e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], dtype=torch.float64)\n",
      "tensor(0.1321, dtype=torch.float64)\n",
      "tensor(0.3176, dtype=torch.float64)\n",
      "tensor([3, 2])\n",
      "tensor([1.7381, 0.1116], dtype=torch.float64)\n",
      "tensor([0.5794, 0.0558], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3176, dtype=torch.float64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mask test!!!\n",
    "noisedints=torch.tensor([[[1,2,1,0],\n",
    "                        [1,4,0,1],\n",
    "                        [0,0,10,0],\n",
    "                        [10,0,0,0],[0,10,0,0],[0,0,10,0],[10,0,0,0]],\n",
    "                        [[1,2,1,0],\n",
    "                        [1,4,0,1],\n",
    "                        [0,0,10,0],\n",
    "                        [10,0,0,0],[0,10,0,0],[0,0,10,0],[10,0,0,0]]],\n",
    "                        dtype=float)\n",
    "noised_labels=torch.tensor([[0,1,2,3,3,3,3],[0,1,2,3,3,3,3]])\n",
    "noised_onehot=nn.functional.one_hot(noised_labels)\n",
    "mask=torch.tensor([[[1],[1],[1],[0],[0],[0],[0]],[[0],[1],[1],[0],[0],[0],[0]]])\n",
    "print(noisedints.size(), noised_labels.size(), noised_onehot.size(),mask.size())\n",
    "\n",
    "loss =mycross_OH_3Dto3D(noisedints,noised_onehot.to(torch.float64))\n",
    "print(loss)\n",
    "\n",
    "masked = loss * mask[:, :, 0]\n",
    "print(masked) # == cros\n",
    "print(torch.mean(masked))\n",
    "\n",
    "print(my_masked_mean(loss,mask))\n",
    "mask_batchs=torch.sum(mask[:,:,0],dim=1)\n",
    "print(mask_batchs)\n",
    "print(torch.sum(masked,dim=1))\n",
    "print(torch.sum(masked,dim=1)/ mask_batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(nn.functional.one_hot(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e1e1ab-f44c-4845-8976-9d4c4e76f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/my_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def recursiveTrace(obj):\n",
    "  print(type(obj))\n",
    "  if hasattr(obj, 'grad_fn'):\n",
    "    print(obj.grad_fn)\n",
    "    recursiveTrace(obj.grad_fn)\n",
    "  elif hasattr(obj, 'saved_variables'):\n",
    "    print(obj.requires_grad, len(obj.saved_tensors), len(obj.saved_variables))\n",
    "    [print(v) for v in obj.saved_variables]\n",
    "    [recursiveTrace(v.grad_fn) for v in obj.saved_variables]\n",
    "\n",
    "\n",
    "def cuda(x, grad=False, gpu_id=-1):\n",
    "  x = x.float() if T.is_tensor(x) else x\n",
    "  if gpu_id == -1:\n",
    "    t = T.FloatTensor(x)\n",
    "    t.requires_grad=grad\n",
    "    return t\n",
    "  else:\n",
    "    t = T.FloatTensor(x.pin_memory()).cuda(gpu_id)\n",
    "    t.requires_grad=grad\n",
    "    return t\n",
    "\n",
    "\n",
    "def cudavec(x, grad=False, gpu_id=-1):\n",
    "  if gpu_id == -1:\n",
    "    t = T.Tensor(T.from_numpy(x))\n",
    "    t.requires_grad = grad\n",
    "    return t\n",
    "  else:\n",
    "    t = T.Tensor(T.from_numpy(x).pin_memory()).cuda(gpu_id)\n",
    "    t.requires_grad = grad\n",
    "    return t\n",
    "\n",
    "\n",
    "def cudalong(x, grad=False, gpu_id=-1):\n",
    "  if gpu_id == -1:\n",
    "    t = T.LongTensor(T.from_numpy(x.astype(np.long)))\n",
    "    t.requires_grad = grad\n",
    "    return t\n",
    "  else:\n",
    "    t = T.LongTensor(T.from_numpy(x.astype(np.long)).pin_memory()).cuda(gpu_id)\n",
    "    t.requires_grad = grad\n",
    "    return t\n",
    "\n",
    "\n",
    "def θ(a, b, normBy=2):\n",
    "  \"\"\"Batchwise Cosine similarity\n",
    "  Cosine similarity\n",
    "  Arguments:\n",
    "      a {Tensor} -- A 3D Tensor (b * m * w)\n",
    "      b {Tensor} -- A 3D Tensor (b * r * w)\n",
    "  Returns:\n",
    "      Tensor -- Batchwise cosine similarity (b * r * m)\n",
    "  \"\"\"\n",
    "  dot = T.bmm(a, b.transpose(1,2))\n",
    "  a_norm = T.norm(a, normBy, dim=2).unsqueeze(2)\n",
    "  b_norm = T.norm(b, normBy, dim=2).unsqueeze(1)\n",
    "  cos = dot / (a_norm * b_norm + δ)\n",
    "  return cos.transpose(1,2).contiguous()\n",
    "\n",
    "\n",
    "def σ(input, axis=1):\n",
    "  \"\"\"Softmax on an axis\n",
    "  Softmax on an axis\n",
    "  Arguments:\n",
    "      input {Tensor} -- input Tensor\n",
    "  Keyword Arguments:\n",
    "      axis {number} -- axis on which to take softmax on (default: {1})\n",
    "  Returns:\n",
    "      Tensor -- Softmax output Tensor\n",
    "  \"\"\"\n",
    "  input_size = input.size()\n",
    "\n",
    "  trans_input = input.transpose(axis, len(input_size) - 1)\n",
    "  trans_size = trans_input.size()\n",
    "\n",
    "  input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "  soft_max_2d = F.softmax(input_2d, -1)\n",
    "  soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "  return soft_max_nd.transpose(axis, len(input_size) - 1)\n",
    "\n",
    "δ = 1e-6\n",
    "\n",
    "\n",
    "def register_nan_checks(model):\n",
    "  def check_grad(module, grad_input, grad_output):\n",
    "    # print(module) you can add this to see that the hook is called\n",
    "    # print('hook called for ' + str(type(module)))\n",
    "    if any(np.all(np.isnan(gi.data.cpu().numpy())) for gi in grad_input if gi is not None):\n",
    "      print('NaN gradient in grad_input ' + type(module).__name__)\n",
    "\n",
    "  model.apply(lambda module: module.register_backward_hook(check_grad))\n",
    "\n",
    "\n",
    "def apply_dict(dic):\n",
    "  for k, v in dic.items():\n",
    "    apply_var(v, k)\n",
    "    if isinstance(v, nn.Module):\n",
    "      key_list = [a for a in dir(v) if not a.startswith('__')]\n",
    "      for key in key_list:\n",
    "        apply_var(getattr(v, key), key)\n",
    "      for pk, pv in v._parameters.items():\n",
    "        apply_var(pv, pk)\n",
    "\n",
    "\n",
    "def apply_var(v, k):\n",
    "  if isinstance(v, Variable) and v.requires_grad:\n",
    "    v.register_hook(check_nan_gradient(k))\n",
    "\n",
    "\n",
    "def check_nan_gradient(name=''):\n",
    "  def f(tensor):\n",
    "    if np.isnan(T.mean(tensor).data.cpu().numpy()):\n",
    "      print('\\nnan gradient of {} :'.format(name))\n",
    "      # print(tensor)\n",
    "      # assert 0, 'nan gradient'\n",
    "      return tensor\n",
    "  return f\n",
    "\n",
    "def ptr(tensor):\n",
    "  if T.is_tensor(tensor):\n",
    "    return tensor.storage().data_ptr()\n",
    "  elif hasattr(tensor, 'data'):\n",
    "    return tensor.clone().data.storage().data_ptr()\n",
    "  else:\n",
    "    return tensor\n",
    "\n",
    "# TODO: EWW change this shit\n",
    "def ensure_gpu(tensor, gpu_id):\n",
    "  if \"cuda\" in str(type(tensor)) and gpu_id != -1:\n",
    "    return tensor.cuda(gpu_id)\n",
    "  elif \"cuda\" in str(type(tensor)):\n",
    "    return tensor.cpu()\n",
    "  elif \"Tensor\" in str(type(tensor)) and gpu_id != -1:\n",
    "    return tensor.cuda(gpu_id)\n",
    "  elif \"Tensor\" in str(type(tensor)):\n",
    "    return tensor\n",
    "  elif type(tensor) is np.ndarray:\n",
    "    return cudavec(tensor, gpu_id=gpu_id).data\n",
    "  else:\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def print_gradient(x, name):\n",
    "  s = \"Gradient of \" + name + \" ----------------------------------\"\n",
    "  x.register_hook(lambda y: print(s, y.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3552199-a62d-45a7-8a2e-0eac4f915117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

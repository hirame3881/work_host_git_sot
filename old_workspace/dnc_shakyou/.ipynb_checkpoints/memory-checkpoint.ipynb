{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125f7aec-e063-45f9-b319-d15ce5a4a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/my_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMemory\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, mem_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, cell_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, read_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, independent_linears\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#from .util import *\n",
    "\n",
    "\n",
    "class Memory(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, mem_size=512, cell_size=32, read_heads=4, gpu_id=-1, independent_linears=True):\n",
    "    super(Memory, self).__init__()\n",
    "\n",
    "    self.mem_size = mem_size\n",
    "    self.cell_size = cell_size\n",
    "    self.read_heads = read_heads\n",
    "    self.gpu_id = gpu_id\n",
    "    self.input_size = input_size\n",
    "    self.independent_linears = independent_linears\n",
    "\n",
    "    m = self.mem_size\n",
    "    w = self.cell_size\n",
    "    r = self.read_heads\n",
    "\n",
    "    if self.independent_linears:\n",
    "      self.read_keys_transform = nn.Linear(self.input_size, w * r)\n",
    "      self.read_strengths_transform = nn.Linear(self.input_size, r)\n",
    "      self.write_key_transform = nn.Linear(self.input_size, w)\n",
    "      self.write_strength_transform = nn.Linear(self.input_size, 1)\n",
    "      self.erase_vector_transform = nn.Linear(self.input_size, w)\n",
    "      self.write_vector_transform = nn.Linear(self.input_size, w)\n",
    "      self.free_gates_transform = nn.Linear(self.input_size, r)\n",
    "      self.allocation_gate_transform = nn.Linear(self.input_size, 1)\n",
    "      self.write_gate_transform = nn.Linear(self.input_size, 1)\n",
    "      self.read_modes_transform = nn.Linear(self.input_size, 3 * r)\n",
    "    else:\n",
    "      self.interface_size = (w * r) + (3 * w) + (5 * r) + 3\n",
    "      self.interface_weights = nn.Linear(self.input_size, self.interface_size)\n",
    "\n",
    "    self.I = cuda(1 - T.eye(m).unsqueeze(0), gpu_id=self.gpu_id)  # (1 * n * n)\n",
    "\n",
    "  def reset(self, batch_size=1, hidden=None, erase=True):\n",
    "    m = self.mem_size\n",
    "    w = self.cell_size\n",
    "    r = self.read_heads\n",
    "    b = batch_size\n",
    "\n",
    "    if hidden is None:\n",
    "      return {\n",
    "          'memory': cuda(T.zeros(b, m, w).fill_(0), gpu_id=self.gpu_id),\n",
    "          'link_matrix': cuda(T.zeros(b, 1, m, m), gpu_id=self.gpu_id),\n",
    "          'precedence': cuda(T.zeros(b, 1, m), gpu_id=self.gpu_id),\n",
    "          'read_weights': cuda(T.zeros(b, r, m).fill_(0), gpu_id=self.gpu_id),\n",
    "          'write_weights': cuda(T.zeros(b, 1, m).fill_(0), gpu_id=self.gpu_id),\n",
    "          'usage_vector': cuda(T.zeros(b, m), gpu_id=self.gpu_id)\n",
    "      }\n",
    "    else:\n",
    "      hidden['memory'] = hidden['memory'].clone()\n",
    "      hidden['link_matrix'] = hidden['link_matrix'].clone()\n",
    "      hidden['precedence'] = hidden['precedence'].clone()\n",
    "      hidden['read_weights'] = hidden['read_weights'].clone()\n",
    "      hidden['write_weights'] = hidden['write_weights'].clone()\n",
    "      hidden['usage_vector'] = hidden['usage_vector'].clone()\n",
    "\n",
    "      if erase:\n",
    "        hidden['memory'].data.fill_(0)\n",
    "        hidden['link_matrix'].data.zero_()\n",
    "        hidden['precedence'].data.zero_()\n",
    "        hidden['read_weights'].data.fill_(0)\n",
    "        hidden['write_weights'].data.fill_(0)\n",
    "        hidden['usage_vector'].data.zero_()\n",
    "    return hidden\n",
    "\n",
    "  def get_usage_vector(self, usage, free_gates, read_weights, write_weights):\n",
    "    # write_weights = write_weights.detach()  # detach from the computation graph\n",
    "    usage = usage + (1 - usage) * (1 - T.prod(1 - write_weights, 1))\n",
    "    ψ = T.prod(1 - free_gates.unsqueeze(2) * read_weights, 1)\n",
    "    return usage * ψ\n",
    "\n",
    "  def allocate(self, usage, write_gate):\n",
    "    # ensure values are not too small prior to cumprod.\n",
    "    usage = δ + (1 - δ) * usage\n",
    "    batch_size = usage.size(0)\n",
    "    # free list\n",
    "    sorted_usage, φ = T.topk(usage, self.mem_size, dim=1, largest=False)\n",
    "\n",
    "    # cumprod with exclusive=True\n",
    "    # https://discuss.pytorch.org/t/cumprod-exclusive-true-equivalences/2614/8\n",
    "    v = var(sorted_usage.data.new(batch_size, 1).fill_(1))\n",
    "    cat_sorted_usage = T.cat((v, sorted_usage), 1)\n",
    "    prod_sorted_usage = T.cumprod(cat_sorted_usage, 1)[:, :-1]\n",
    "\n",
    "    sorted_allocation_weights = (1 - sorted_usage) * prod_sorted_usage.squeeze()\n",
    "\n",
    "    # construct the reverse sorting index https://stackoverflow.com/questions/2483696/undo-or-reverse-argsort-python\n",
    "    _, φ_rev = T.topk(φ, k=self.mem_size, dim=1, largest=False)\n",
    "    allocation_weights = sorted_allocation_weights.gather(1, φ_rev.long())\n",
    "\n",
    "    return allocation_weights.unsqueeze(1), usage\n",
    "\n",
    "  def write_weighting(self, memory, write_content_weights, allocation_weights, write_gate, allocation_gate):\n",
    "    ag = allocation_gate.unsqueeze(-1)\n",
    "    wg = write_gate.unsqueeze(-1)\n",
    "\n",
    "    return wg * (ag * allocation_weights + (1 - ag) * write_content_weights)\n",
    "\n",
    "  def get_link_matrix(self, link_matrix, write_weights, precedence):\n",
    "    precedence = precedence.unsqueeze(2)\n",
    "    write_weights_i = write_weights.unsqueeze(3)\n",
    "    write_weights_j = write_weights.unsqueeze(2)\n",
    "\n",
    "    prev_scale = 1 - write_weights_i - write_weights_j\n",
    "    new_link_matrix = write_weights_i * precedence\n",
    "\n",
    "    link_matrix = prev_scale * link_matrix + new_link_matrix\n",
    "    # trick to delete diag elems\n",
    "    return self.I.expand_as(link_matrix) * link_matrix\n",
    "\n",
    "  def update_precedence(self, precedence, write_weights):\n",
    "    return (1 - T.sum(write_weights, 2, keepdim=True)) * precedence + write_weights\n",
    "\n",
    "  def write(self, write_key, write_vector, erase_vector, free_gates, read_strengths, write_strength, write_gate, allocation_gate, hidden):\n",
    "    # get current usage\n",
    "    hidden['usage_vector'] = self.get_usage_vector(\n",
    "        hidden['usage_vector'],\n",
    "        free_gates,\n",
    "        hidden['read_weights'],\n",
    "        hidden['write_weights']\n",
    "    )\n",
    "\n",
    "    # lookup memory with write_key and write_strength\n",
    "    write_content_weights = self.content_weightings(hidden['memory'], write_key, write_strength)\n",
    "\n",
    "    # get memory allocation\n",
    "    alloc, _ = self.allocate(\n",
    "        hidden['usage_vector'],\n",
    "        allocation_gate * write_gate\n",
    "    )\n",
    "\n",
    "    # get write weightings\n",
    "    hidden['write_weights'] = self.write_weighting(\n",
    "        hidden['memory'],\n",
    "        write_content_weights,\n",
    "        alloc,\n",
    "        write_gate,\n",
    "        allocation_gate\n",
    "    )\n",
    "\n",
    "    weighted_resets = hidden['write_weights'].unsqueeze(3) * erase_vector.unsqueeze(2)\n",
    "    reset_gate = T.prod(1 - weighted_resets, 1)\n",
    "    # Update memory\n",
    "    hidden['memory'] = hidden['memory'] * reset_gate\n",
    "\n",
    "    hidden['memory'] = hidden['memory'] + \\\n",
    "        T.bmm(hidden['write_weights'].transpose(1, 2), write_vector)\n",
    "\n",
    "    # update link_matrix\n",
    "    hidden['link_matrix'] = self.get_link_matrix(\n",
    "        hidden['link_matrix'],\n",
    "        hidden['write_weights'],\n",
    "        hidden['precedence']\n",
    "    )\n",
    "    hidden['precedence'] = self.update_precedence(hidden['precedence'], hidden['write_weights'])\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def content_weightings(self, memory, keys, strengths):\n",
    "    d = θ(memory, keys)\n",
    "    return σ(d * strengths.unsqueeze(2), 2)\n",
    "\n",
    "  def directional_weightings(self, link_matrix, read_weights):\n",
    "    rw = read_weights.unsqueeze(1)\n",
    "\n",
    "    f = T.matmul(link_matrix, rw.transpose(2, 3)).transpose(2, 3)\n",
    "    b = T.matmul(rw, link_matrix)\n",
    "    return f.transpose(1, 2), b.transpose(1, 2)\n",
    "\n",
    "  def read_weightings(self, memory, content_weights, link_matrix, read_modes, read_weights):\n",
    "    forward_weight, backward_weight = self.directional_weightings(link_matrix, read_weights)\n",
    "\n",
    "    content_mode = read_modes[:, :, 2].contiguous().unsqueeze(2) * content_weights\n",
    "    backward_mode = T.sum(read_modes[:, :, 0:1].contiguous().unsqueeze(3) * backward_weight, 2)\n",
    "    forward_mode = T.sum(read_modes[:, :, 1:2].contiguous().unsqueeze(3) * forward_weight, 2)\n",
    "\n",
    "    return backward_mode + content_mode + forward_mode\n",
    "\n",
    "  def read_vectors(self, memory, read_weights):\n",
    "    return T.bmm(read_weights, memory)\n",
    "\n",
    "  def read(self, read_keys, read_strengths, read_modes, hidden):\n",
    "    content_weights = self.content_weightings(hidden['memory'], read_keys, read_strengths)\n",
    "\n",
    "    hidden['read_weights'] = self.read_weightings(\n",
    "        hidden['memory'],\n",
    "        content_weights,\n",
    "        hidden['link_matrix'],\n",
    "        read_modes,\n",
    "        hidden['read_weights']\n",
    "    )\n",
    "    read_vectors = self.read_vectors(hidden['memory'], hidden['read_weights'])\n",
    "    return read_vectors, hidden\n",
    "\n",
    "  def forward(self, ξ, hidden):\n",
    "\n",
    "    # ξ = ξ.detach()\n",
    "    m = self.mem_size\n",
    "    w = self.cell_size\n",
    "    r = self.read_heads\n",
    "    b = ξ.size()[0]\n",
    "\n",
    "    if self.independent_linears:\n",
    "      # r read keys (b * r * w)\n",
    "      read_keys = T.tanh(self.read_keys_transform(ξ).view(b, r, w))\n",
    "      # r read strengths (b * r)\n",
    "      read_strengths = F.softplus(self.read_strengths_transform(ξ).view(b, r))\n",
    "      # write key (b * 1 * w)\n",
    "      write_key = T.tanh(self.write_key_transform(ξ).view(b, 1, w))\n",
    "      # write strength (b * 1)\n",
    "      write_strength = F.softplus(self.write_strength_transform(ξ).view(b, 1))\n",
    "      # erase vector (b * 1 * w)\n",
    "      erase_vector = T.sigmoid(self.erase_vector_transform(ξ).view(b, 1, w))\n",
    "      # write vector (b * 1 * w)\n",
    "      write_vector = T.tanh(self.write_vector_transform(ξ).view(b, 1, w))\n",
    "      # r free gates (b * r)\n",
    "      free_gates = T.sigmoid(self.free_gates_transform(ξ).view(b, r))\n",
    "      # allocation gate (b * 1)\n",
    "      allocation_gate = T.sigmoid(self.allocation_gate_transform(ξ).view(b, 1))\n",
    "      # write gate (b * 1)\n",
    "      write_gate = T.sigmoid(self.write_gate_transform(ξ).view(b, 1))\n",
    "      # read modes (b * r * 3)\n",
    "      read_modes = σ(self.read_modes_transform(ξ).view(b, r, 3), -1)\n",
    "    else:\n",
    "      ξ = self.interface_weights(ξ)\n",
    "      # r read keys (b * w * r)\n",
    "      read_keys = T.tanh(ξ[:, :r * w].contiguous().view(b, r, w))\n",
    "      # r read strengths (b * r)\n",
    "      read_strengths = F.softplus(ξ[:, r * w:r * w + r].contiguous().view(b, r))\n",
    "      # write key (b * w * 1)\n",
    "      write_key = T.tanh(ξ[:, r * w + r:r * w + r + w].contiguous().view(b, 1, w))\n",
    "      # write strength (b * 1)\n",
    "      write_strength = F.softplus(ξ[:, r * w + r + w].contiguous().view(b, 1))\n",
    "      # erase vector (b * w)\n",
    "      erase_vector = T.sigmoid(ξ[:, r * w + r + w + 1: r * w + r + 2 * w + 1].contiguous().view(b, 1, w))\n",
    "      # write vector (b * w)\n",
    "      write_vector = T.tanh(ξ[:, r * w + r + 2 * w + 1: r * w + r + 3 * w + 1].contiguous().view(b, 1, w))\n",
    "      # r free gates (b * r)\n",
    "      free_gates = T.sigmoid(ξ[:, r * w + r + 3 * w + 1: r * w + 2 * r + 3 * w + 1].contiguous().view(b, r))\n",
    "      # allocation gate (b * 1)\n",
    "      allocation_gate = T.sigmoid(ξ[:, r * w + 2 * r + 3 * w + 1].contiguous().unsqueeze(1).view(b, 1))\n",
    "      # write gate (b * 1)\n",
    "      write_gate = T.sigmoid(ξ[:, r * w + 2 * r + 3 * w + 2].contiguous()).unsqueeze(1).view(b, 1)\n",
    "      # read modes (b * 3*r)\n",
    "      read_modes = σ(ξ[:, r * w + 2 * r + 3 * w + 3: r * w + 5 * r + 3 * w + 3].contiguous().view(b, r, 3), -1)\n",
    "\n",
    "    hidden = self.write(write_key, write_vector, erase_vector, free_gates,\n",
    "                        read_strengths, write_strength, write_gate, allocation_gate, hidden)\n",
    "    return self.read(read_keys, read_strengths, read_modes, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525f6a2-1b39-4966-958e-325e5b73146b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

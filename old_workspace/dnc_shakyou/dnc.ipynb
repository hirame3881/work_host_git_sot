{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef5d5a8-ed38-4164-94f7-6626da0e4480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/my_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importnb\n",
    "with __import__('importnb').Notebook(): \n",
    "    from memory import *\n",
    "    from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67884e1b-dc1f-4bac-8b77-7b52e79b9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as pad\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "from torch.nn.init import orthogonal_, xavier_uniform_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5910ee15-6ef8-45b2-ac1e-889999a73b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        rnn_type=\"lstm\",\n",
    "        num_layers=1,\n",
    "        num_hidden_layers=2,\n",
    "        bias=True,\n",
    "        batch_first=True,\n",
    "        dropout=0,\n",
    "        bidirectional=False, #~?\n",
    "        nr_cells=5,\n",
    "        read_heads=2,\n",
    "        cell_size=10,\n",
    "        nonlinearity=\"tanh\",\n",
    "        gpu_id=-1,\n",
    "        independent_linears=False,\n",
    "        share_memory=True,\n",
    "        debug=False,\n",
    "        clip=20\n",
    "    ):\n",
    "    \n",
    "        super(DNC, self).__init__()\n",
    "        #todo: separate weights and RNNs for the interface and output vectors\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nr_cells = nr_cells\n",
    "        self.read_heads = read_heads\n",
    "        self.cell_size = cell_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.gpu_id = gpu_id\n",
    "        self.independent_linears = independent_linears\n",
    "        self.share_memory = share_memory\n",
    "        self.debug = debug\n",
    "        self.clip = clip\n",
    "        \n",
    "        self.w = self.cell_size\n",
    "        self.r = self.read_heads\n",
    "        \n",
    "        self.read_vectors_size = self.r * self.w\n",
    "        self.output_size = self.hidden_size\n",
    "        \n",
    "        self.nn_input_size = self.input_size + self.read_vectors_size\n",
    "        self.nn_output_size = self.output_size + self.read_vectors_size\n",
    "        \n",
    "        self.rnns = []\n",
    "        self.memories = []\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            if self.rnn_type.lower() == 'rnn':\n",
    "                self.rnns.append(nn.RNN( (self.nn_input_size if layer==0 else self.nn_output_size), self.output_size,\n",
    "                                        bias=self.bias,nonlinearity=self.nonlinearity, batch_first=True, dropout=self.dropout, num_layers=self.num_hidden_layers))\n",
    "            elif self.rnn_type.lower() == 'gru':\n",
    "                self.rnns.append(nn.GRU((self.nn_input_size if layer==0 else self.nn_output_size), self.output_size,\n",
    "                                        bias=self.bias, batch_first=True, dropout=self.dropout, num_layers=self.num_hidden_layers))\n",
    "            if self.rnn_type.lower() == 'lstm':\n",
    "                self.rnns.append(nn.LSTM((self.nn_input_size if layer==0 else self.nn_output_size), self.output_size,\n",
    "                                        bias=self.bias, batch_first=True, dropout=self.dropout, num_layers=self.num_hidden_layers))\n",
    "            setattr(self,self.rnn_type.lower() + '_layer_' + str(layer), self.rnns[layer])   \n",
    "            \n",
    "            #memories for each layer\n",
    "            if not self.share_memory:\n",
    "                self.memories.append(\n",
    "                    Memory(\n",
    "                        input_size=self.output_size,\n",
    "                        mem_size=self.nr_cells,\n",
    "                        cell_size=self.w,\n",
    "                        read_heads=self.r,\n",
    "                        gpu_id=self.gpu_id,\n",
    "                        independent_linears=self.independent_linears\n",
    "                    )\n",
    "                )\n",
    "                setattr(self, 'rnn_layer_memory_' + str(layer),self.memories[layer])\n",
    "            \n",
    "        # only one mory shared by all layers\n",
    "        if self.share_memory:\n",
    "            self.memories.append(\n",
    "                Memory(\n",
    "                    input_size=self.output_size,\n",
    "                    mem_size=self.nr_cells,\n",
    "                    cell_size=self.w,\n",
    "                    read_heads=self.r,\n",
    "                    gpu_id=self.gpu_id,\n",
    "                    independent_linears=self.independent_linears\n",
    "                )\n",
    "            )\n",
    "            setattr(self, 'rnn_layer_memory_shared' ,self.memories[0])\n",
    "        \n",
    "        #final output layer\n",
    "        self.output = nn.Linear(self.nn_output_size, self.input_size)\n",
    "        orthogonal_(self.output.weight)\n",
    "        \n",
    "        if self.gpu_id != -1:\n",
    "            [x.cuda(self.gpu_id) for x in self.rnns]\n",
    "            [x.cuda(self.gpu_id) for x in self.memories]\n",
    "            self.output.cuda()\n",
    "            \n",
    "            \n",
    "    def _init_hidden(self, hx, batch_size, reset_experience):\n",
    "        #create empty hidden states if not provided\n",
    "        if hx is None:\n",
    "            hx= (None,None,None)\n",
    "        (chx,mhx,last_read) =hx\n",
    "        \n",
    "        # initialize hidden state of the controller RNN\n",
    "        if chx is None:\n",
    "            h=cuda(T.zeros(self.num_hidden_layers, batch_size, self.output_size), gpu_id=self.gpu_id)\n",
    "            xavier_uniform_(h)\n",
    "            \n",
    "            chx=[ (h,h) if self.rnn_type.lower() =='lstm' else h for x in range(self.num_layers)]\n",
    "        \n",
    "        #Last read vectors\n",
    "        if last_read is None:\n",
    "            last_read = cuda(T.zeros(batch_size, self.w * self.r), gpu_id=self.gpu_id) #~?\n",
    "        \n",
    "        #memory states\n",
    "        if mhx is None:\n",
    "            if self.share_memory:\n",
    "                mhx=self.memories[0].reset(batch_size, erase=reset_experience)\n",
    "            else:\n",
    "                mhx=[m.reset(batch_size, erase=reset_experience) for m in self.memories]\n",
    "        else:\n",
    "            if self.share_memory:\n",
    "                mhx=self.memories[0].reset(batch_size,mhx,erase=reset_experience)\n",
    "            else:\n",
    "                mhx=[m.reset(batch_size,h, erase=reset_experience) for m,h in zip(self.memories,mhx)]\n",
    "        \n",
    "        return chx,mhx,last_read\n",
    "    \n",
    "    \n",
    "    def _debug(self,mhx,debug_obj):\n",
    "        if not debug_obj:\n",
    "            debug_obj = {\n",
    "                'memory':[],\n",
    "                'link_matrix':[],\n",
    "                'precedence':[],\n",
    "                'read_weights':[],\n",
    "                'write_weights':[],\n",
    "                'usage_vector':[],\n",
    "            }\n",
    "        \n",
    "        debug_obj['memory'].append(mhx['memory'][0].data.cpu().numpy())\n",
    "        #省略　メモリ実装したらまた戻ってくる\n",
    "        debug_obj['link_matrix'].append(mhx['link_matrix'][0][0].data.cpu().numpy())\n",
    "        debug_obj['precedence'].append(mhx['precedence'][0].data.cpu().numpy())\n",
    "        debug_obj['read_weights'].append(mhx['read_weights'][0].data.cpu().numpy())\n",
    "        debug_obj['write_weights'].append(mhx['write_weights'][0].data.cpu().numpy())\n",
    "        debug_obj['usage_vector'].append(mhx['usage_vector'][0].unsqueeze(0).data.cpu().numpy())\n",
    "        return debug_obj\n",
    "\n",
    "        \n",
    "        \n",
    "    def _layer_forward(self, input, layer, hx=(None,None), pass_through_memory=True):\n",
    "        (chx, mhx) = hx\n",
    "        \n",
    "        #pass through the controller layer\n",
    "        input, chx = self.rnns[layer](input.unsqueeze(1),chx)\n",
    "        input = input.squeeze(1)\n",
    "        \n",
    "        #clip the controller output\n",
    "        if self.clip !=0:\n",
    "            output= T.clamp(input, -self.clip, self.clip)\n",
    "        else: \n",
    "            output=input\n",
    "        \n",
    "        # the interface vector\n",
    "        ξ = output\n",
    "        \n",
    "        # pass through memory\n",
    "        if pass_through_memory:\n",
    "            if self.share_memory:\n",
    "                read_vecs, mhx = self.memories[0](ξ,mhx)\n",
    "            else:\n",
    "                read_vecs, mhx = self.memories[layer](ξ,mhx)\n",
    "            \n",
    "            #the read vectors\n",
    "            read_vectors = read_vecs.view(-1, self.w * self.r)\n",
    "        \n",
    "        else:\n",
    "            read_vectors = None\n",
    "            \n",
    "        return output, (chx, mhx, read_vectors)\n",
    "    \n",
    "    \n",
    "    def forward(self,input, hx=(None,None,None), reset_experience =False, pass_through_memory=True):\n",
    "        # handle packed data\n",
    "        is_packed = type(input) is PackedSequence\n",
    "        if is_packed:\n",
    "            input,lengths = pad(input)\n",
    "            max_length = lengths[0]\n",
    "        else:\n",
    "            max_length = input.size(1) if self.batch_first else input.size(0)\n",
    "            lengths = [input.size(1)] * max_length if self.batch_first else [input.size(0)] * max_length\n",
    "        \n",
    "        batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            input=input.transpose(0,1)\n",
    "        # make the data time-first\n",
    "        \n",
    "        controller_hidden, mem_hidden, last_read = self._init_hidden(hx, batch_size, reset_experience)\n",
    "        \n",
    "        # concat input with last read (or padding) vectors\n",
    "        #print(input.device)\n",
    "        #print(last_read.device)\n",
    "        inputs = [T.cat([input[:,x,:],last_read],1) for x in range(max_length)]\n",
    "        \n",
    "        #batched forward pass per element / word / etc\n",
    "        if self.debug:\n",
    "            viz=None\n",
    "            \n",
    "        outs =[None] * max_length\n",
    "        read_vectors = None\n",
    "        \n",
    "        # pass through time\n",
    "        for time in range(max_length):\n",
    "            # pass thorugh layers\n",
    "            for layer in range(self.num_layers):\n",
    "                #this layer's hidden states\n",
    "                chx = controller_hidden[layer]\n",
    "                m = mem_hidden if self.share_memory else mem_hidden[layer]\n",
    "                # pass through controller\n",
    "                outs[time], (chx,m,read_vectors) = \\\n",
    "                    self._layer_forward(inputs[time],layer,(chx,m), pass_through_memory)\n",
    "                \n",
    "                #debug memory\n",
    "                if self.debug:\n",
    "                    viz = self._debug(m,viz)\n",
    "                    \n",
    "                #store the memory back (per layer or shared)\n",
    "                if self.share_memory:\n",
    "                    mem_hidden=m\n",
    "                else:\n",
    "                    mem_hidden[layer] =m\n",
    "                controller_hidden[layer] =chx\n",
    "                \n",
    "                if read_vectors is not None:\n",
    "                    # the controller output + read vectors go into next layer\n",
    "                    outs[time] = T.cat ([outs[time],read_vectors],1)         #~\n",
    "                else:\n",
    "                    outs[time] = T.cat([outs[time],last_read],1)\n",
    "                inputs[time] =outs[time]\n",
    "                \n",
    "        if self.debug:\n",
    "            viz ={k: np.array(v) for k,v in viz.items()}\n",
    "            viz ={k: v.reshape(v.shape[0], v.shape[1] * v.shape[2]) for k,v in viz.items()}\n",
    "\n",
    "        # pass through final output layer\n",
    "        inputs = [self.output(i) for i in inputs]\n",
    "        outputs = T.stack(inputs, 1 if self.batch_first else 0)\n",
    "\n",
    "        if is_packed:\n",
    "            outputs = pack(output,lengths)\n",
    "\n",
    "        if self.debug:\n",
    "            return outputs, (controller_hidden, mem_hidden, read_vectors), viz\n",
    "        else:\n",
    "            return outputs, (controller_hidden, mem_hidden, read_vectors)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = \"\\n----------------------------------------\\n\"\n",
    "        s += '{name}({input_size}, {hidden_size}'\n",
    "        if self.rnn_type != 'lstm':\n",
    "          s += ', rnn_type={rnn_type}'\n",
    "        if self.num_layers != 1:\n",
    "          s += ', num_layers={num_layers}'\n",
    "        if self.num_hidden_layers != 2:\n",
    "          s += ', num_hidden_layers={num_hidden_layers}'\n",
    "        if self.bias != True:\n",
    "          s += ', bias={bias}'\n",
    "        if self.batch_first != True:\n",
    "          s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "          s += ', dropout={dropout}'\n",
    "        if self.bidirectional != False:\n",
    "          s += ', bidirectional={bidirectional}'\n",
    "        if self.nr_cells != 5:\n",
    "          s += ', nr_cells={nr_cells}'\n",
    "        if self.read_heads != 2:\n",
    "          s += ', read_heads={read_heads}'\n",
    "        if self.cell_size != 10:\n",
    "          s += ', cell_size={cell_size}'\n",
    "        if self.nonlinearity != 'tanh':\n",
    "          s += ', nonlinearity={nonlinearity}'\n",
    "        if self.gpu_id != -1:\n",
    "          s += ', gpu_id={gpu_id}'\n",
    "        if self.independent_linears != False:\n",
    "          s += ', independent_linears={independent_linears}'\n",
    "        if self.share_memory != True:\n",
    "          s += ', share_memory={share_memory}'\n",
    "        if self.debug != False:\n",
    "          s += ', debug={debug}'\n",
    "        if self.clip != 20:\n",
    "          s += ', clip={clip}'\n",
    "\n",
    "        s += \")\\n\" + super(DNC, self).__repr__() + \\\n",
    "          \"\\n----------------------------------------\\n\"\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03af5d3-edea-4509-a3a6-f4a47934c11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f309ee-a304-41aa-89ca-3632143dc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "\n",
    "import os\n",
    "from io import open\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import BABI20\n",
    "from torchtext.data import Dataset, Field, Example, Iterator\n",
    "\n",
    "import argparse\n",
    "from torchtext import datasets\n",
    "from torchtext.datasets.babi import BABI20Field\n",
    "#from models.UTransformer import BabiUTransformer\n",
    "#from models.common_layer import NoamOpt\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "from visdom import Visdom\n",
    "\n",
    "sys.path.insert(0, os.path.join('..', '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f969bb-dd2e-49ce-9eb3-4257cae8cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df9bd80-0dba-4e2e-9de2-713d8193dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with __import__('importnb').Notebook(): \n",
    "    from dnc import DNC\n",
    "    from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44bf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangestory(batch_story): # (batch,length,size)\n",
    "    strysum =torch.sum(batch_story,(0,2)) #1D\n",
    "    fill_length=len(strysum[torch.where(strysum!=0)])\n",
    "    red_story=batch_story[:,:fill_length,:]\n",
    "    red_story =torch.flip(red_story, dims=[1])\n",
    "    flat_story =red_story.view(len(batch_story),-1)\n",
    "    return flat_story #(batch, red_length * size ) 2D\n",
    "\n",
    "def prep_data(batch,vocab_size):\n",
    "    batch_size=batch.story.size()[0]\n",
    "    flat_story =arrangestory(batch.story)\n",
    "    story_OH =torch.nn.functional.one_hot(flat_story,num_classes=vocab_size) #(batch, seq_len, vocab_len)\n",
    "    query_OH=torch.nn.functional.one_hot(batch.query,num_classes=vocab_size) #(batch, que_len=3, vocab_len)\n",
    "    answer_OH=torch.nn.functional.one_hot(batch.answer,num_classes=vocab_size) #(batch,1, vocab_len)\n",
    "    querystop =torch.nn.functional.one_hot(torch.tensor([vocab_size-1]*batch_size),num_classes=vocab_size)\n",
    "    querystop=querystop.view(batch_size,1,-1) #2D -> 3D\n",
    "    x =torch.cat((story_OH,query_OH,querystop),1)\n",
    "    y =answer_OH.view(batch_size,vocab_size)\n",
    "    ans_id =batch.answer.view(-1) #1D (batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    #x = var(x.detach().clone().to(torch.float).cuda() )\n",
    "    #y = var(y.detach().clone().to(torch.float).cuda() )\n",
    "    x = var(x.detach().clone().to(torch.float).cuda() )\n",
    "    y = var(y.detach().clone().to(torch.float).cuda() )\n",
    "    ans_id=var(ans_id.cuda() )\n",
    "    #ans_id=var(ans_id.detach().clone().cuda() )\n",
    "\n",
    "    return x,y,ans_id #yはonehotなので2Dだけどans_idは1D !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2151d042-e7ed-4232-9933-feb28efb7250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-visdom'], dest='visdom', nargs=0, const=True, default=False, type=None, choices=None, help='plot memory content on visdom per -summarize_freq steps', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Differentiable Neural Computer')\n",
    "parser.add_argument('-input_size', type=int, default=6, help='dimension of input feature')\n",
    "parser.add_argument('-rnn_type', type=str, default='lstm', help='type of recurrent cells to use for the controller')\n",
    "parser.add_argument('-nhid', type=int, default=64, help='number of hidden units of the inner nn')\n",
    "parser.add_argument('-dropout', type=float, default=0, help='controller dropout')\n",
    "parser.add_argument('-memory_type', type=str, default='dnc', help='dense or sparse memory: dnc | sdnc | sam')\n",
    "\n",
    "parser.add_argument('-nlayer', type=int, default=1, help='number of layers')\n",
    "parser.add_argument('-nhlayer', type=int, default=2, help='number of hidden layers')\n",
    "parser.add_argument('-lr', type=float, default=1e-4, help='initial learning rate')\n",
    "parser.add_argument('-optim', type=str, default='adam', help='learning rule, supports adam|rmsprop')\n",
    "parser.add_argument('-clip', type=float, default=50, help='gradient clipping')\n",
    "\n",
    "parser.add_argument('-batch_size', type=int, default=100, metavar='N', help='batch size')\n",
    "parser.add_argument('-mem_size', type=int, default=20, help='memory dimension')\n",
    "parser.add_argument('-mem_slot', type=int, default=16, help='number of memory slots')\n",
    "parser.add_argument('-read_heads', type=int, default=4, help='number of read heads')\n",
    "parser.add_argument('-sparse_reads', type=int, default=10, help='number of sparse reads per read head')\n",
    "parser.add_argument('-temporal_reads', type=int, default=2, help='number of temporal reads')\n",
    "\n",
    "parser.add_argument('-sequence_max_length', type=int, default=4, metavar='N', help='sequence_max_length')\n",
    "parser.add_argument('-curriculum_increment', type=int, default=0, metavar='N', help='sequence_max_length incrementor per 1K iterations')\n",
    "parser.add_argument('-curriculum_freq', type=int, default=1000, metavar='N', help='sequence_max_length incrementor per 1K iterations')\n",
    "parser.add_argument('-cuda', type=int, default=-1, help='Cuda GPU ID, -1 for CPU')\n",
    "\n",
    "parser.add_argument('-iterations', type=int, default=100000, metavar='N', help='total number of iteration')\n",
    "parser.add_argument('-summarize_freq', type=int, default=100, metavar='N', help='summarize frequency')\n",
    "parser.add_argument('-check_freq', type=int, default=100, metavar='N', help='check point frequency')\n",
    "parser.add_argument('-visdom', action='store_true', help='plot memory content on visdom per -summarize_freq steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4355e24-db10-4759-a810-f5a0ab39f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=\"-rnn_type lstm -nhid 256 -lr 1e-4 -optim rmsprop -batch_size 100 -mem_size 64 -mem_slot 256 -sequence_max_length 50 -cuda 0 -iterations 1000 -summarize_freq 10 -check_freq 1000\".split(\" \"))\n",
    "\n",
    "#viz = Visdom()\n",
    "# assert viz.check_connection()\n",
    "\n",
    "if args.cuda != -1:\n",
    "  print('Using CUDA.')\n",
    "  #T.manual_seed(1111)\n",
    "else:\n",
    "  print('Using CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1da84b-d96a-4b6b-bb06-833d619407eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llprint(message):\n",
    "  sys.stdout.write(message)\n",
    "  sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_babidata_task_etc(task,batch_size, length, size): #~size\n",
    "    \"\"\"(batch_size=config.batch_size, \n",
    "                                                            root='.data', \n",
    "                                                            memory_size=70, \n",
    "                                                            task=config.task, \n",
    "                                                            joint=False,\n",
    "                                                            tenK=False, \n",
    "                                                            only_supporting=False, \n",
    "                                                            sort=False, \n",
    "                                                            shuffle=True)\"\"\"\n",
    "    text = BABI20Field(length)\n",
    "    train, val, test = datasets.BABI20.splits(text, root='.data', task=task, joint=False,\n",
    "                                            tenK=True, only_supporting=False)\n",
    "    text.build_vocab(train)\n",
    "    #text.vocab.append_token\n",
    "    vocab_len = len(text.vocab.freqs) +1\n",
    "    vocab_lenplus =vocab_len+1 # \"?\"\n",
    "    print(\"VOCAB LEN PLUS:\",vocab_lenplus )\n",
    "    train_iter,val_iter,test_iter = Iterator.splits((train, val, test),batch_size=batch_size)\n",
    "    return train_iter, val_iter, test_iter,vocab_lenplus,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d467ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:  tensor([[6, 2, 7, 1, 3, 5, 6, 2, 0, 4, 5, 2, 7, 1, 2, 0, 3, 5, 0, 4],\n",
      "        [0, 3, 5, 5, 5, 5, 2, 0, 3, 7, 0, 3, 6, 0, 1, 2, 1, 6, 7, 7],\n",
      "        [5, 2, 7, 4, 0, 6, 3, 6, 3, 1, 6, 2, 5, 3, 0, 6, 1, 6, 7, 1]],\n",
      "       device='cuda:0')\n",
      "input:   tensor([[[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]], device='cuda:0')\n",
      "target:   tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Data preparetion\n",
    "def generate_copyfirst(batch_size=3,length=20,vocab_size=10):\n",
    "    vocab_size-=1\n",
    "    sequence = np.random.randint(0,vocab_size-1,(batch_size,length))\n",
    "    input_story= np.zeros((batch_size, length + 1, vocab_size+1), dtype=np.float32)\n",
    "    #input_query=\n",
    "    target_out = np.zeros((batch_size, length + 1, vocab_size+1), dtype=np.float32)\n",
    "\n",
    "    input_story=torch.from_numpy(input_story).cuda()\n",
    "    target_out=torch.from_numpy(target_out).cuda()\n",
    "    sequence=torch.from_numpy(sequence).cuda()\n",
    "\n",
    "    input_story[:,:length,:vocab_size]=torch.nn.functional.one_hot(sequence,num_classes=vocab_size)\n",
    "    input_story[:, length, -1] = 1  # QUERY\n",
    "    target_out[:, -1, :vocab_size] = torch.nn.functional.one_hot(sequence[:,0],num_classes=vocab_size)\n",
    "\n",
    "    return var(input_story),var(target_out),var(sequence) #(batch, length+1, v_size+1)\n",
    "\n",
    "i,t,s =generate_copyfirst()\n",
    "print(\"sequence: \",s)\n",
    "print(\"input:  \",i)\n",
    "print(\"target:  \",t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc8300b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4611502885818481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc=nn.CrossEntropyLoss()\n",
    "cc(t[:,-1,:],s[:,0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a7a709-6991-4d2a-b409-6082e8628b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_copy(task):\n",
    "  print(\"==========task:\",task)\n",
    "  dirname = os.path.dirname(os.path.abspath('__file__'))\n",
    "  ckpts_dir = 'checkpoints'\n",
    "  if not os.path.isdir(ckpts_dir):\n",
    "    os.mkdir(ckpts_dir)\n",
    "  \n",
    "  sequence_max_length = args.sequence_max_length\n",
    "  check_freq = args.check_freq\n",
    "\n",
    "  # input_size = output_size = args.input_size\n",
    "  mem_slot = args.mem_slot\n",
    "  mem_size = args.mem_size\n",
    "  read_heads = args.read_heads\n",
    "\n",
    "  print(\"batch_size:\",args.batch_size)\n",
    "  train_iter, val_iter, test_iter, vocab_size, text_field =get_babidata_task_etc(task,args.batch_size,sequence_max_length,6)\n",
    "  train_iter =[]\n",
    "  batch_size =args.batch_size\n",
    "  batch_iteration_size =9000//batch_size\n",
    "  for i in range(batch_iteration_size):\n",
    "    train_iter.append(list(generate_copyfirst(batch_size=batch_size,length=20,vocab_size=vocab_size)))\n",
    "\n",
    "  if args.memory_type == 'dnc':\n",
    "    rnn = DNC(\n",
    "        #input_size=args.input_size,\n",
    "        input_size=vocab_size,\n",
    "        hidden_size=args.nhid,\n",
    "        rnn_type=args.rnn_type,\n",
    "        num_layers=args.nlayer,\n",
    "        num_hidden_layers=args.nhlayer,\n",
    "        dropout=args.dropout,\n",
    "        nr_cells=mem_slot,\n",
    "        cell_size=mem_size,\n",
    "        read_heads=read_heads,\n",
    "        gpu_id=args.cuda,\n",
    "        debug=args.visdom,\n",
    "        batch_first=True,\n",
    "        independent_linears=True)\n",
    "  #batchfirst\n",
    "  print(rnn)\n",
    "  # register_nan_checks(rnn)\n",
    "\n",
    "  if args.optim == 'adam':\n",
    "    optimizer = optim.Adam(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
    "  elif args.optim == 'adamax':\n",
    "    optimizer = optim.Adamax(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
    "  elif args.optim == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(rnn.parameters(), lr=args.lr, momentum=0.9, eps=1e-10) # 0.0001\n",
    "  elif args.optim == 'sgd':\n",
    "    optimizer = optim.SGD(rnn.parameters(), lr=args.lr) # 0.01\n",
    "  elif args.optim == 'adagrad':\n",
    "    optimizer = optim.Adagrad(rnn.parameters(), lr=args.lr)\n",
    "  elif args.optim == 'adadelta':\n",
    "    optimizer = optim.Adadelta(rnn.parameters(), lr=args.lr)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  #criterion = my_criterion\n",
    "  save_dir=\"dnc_param\"\n",
    "  save_path=os.path.join(save_dir,\"task\"+str(task)+\".pth\")\n",
    "\n",
    "  iterations = args.iterations\n",
    "  iterations=1000\n",
    "  summarize_freq = args.summarize_freq\n",
    "  print(\"summarize_freq:\",summarize_freq)\n",
    "  last_save_losses = []\n",
    "  last_val_losses =[]\n",
    "  val_acc=[]\n",
    "  val_bestacc=0\n",
    "  val_bestloss=0\n",
    "  bestepoch =0\n",
    "\n",
    "  if args.cuda != -1:\n",
    "    rnn = rnn.cuda(args.cuda)\n",
    "  \n",
    "  rnn.train()\n",
    "  (chx, mhx, rv) = (None, None, None)\n",
    "  for epoch in range(iterations + 1):\n",
    "    llprint(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "    \n",
    "    #generate_data(batch_size, random_length, args.input_size, args.cuda)\n",
    "    for i,batch in enumerate(train_iter):\n",
    "      llprint(\"\\rbatch {bt}\".format(bt=i))\n",
    "      train_x,train_y,sequence =batch\n",
    "      #torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "      #if rnn.debug:\n",
    "      #  output, (chx, mhx, rv), v = rnn(train_x, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "      #else:\n",
    "      output, (chx, mhx, rv) = rnn(train_x, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      last_output =output[:,-1,:]\n",
    "      #print(\"last_output.size():\",last_output.size(),\" ans_id.size():\",ans_id.size())\n",
    "      #loss = criterion((last_output),train_y)\n",
    "      loss = criterion(last_output,sequence[:,0])\n",
    "      loss.backward()\n",
    "      T.nn.utils.clip_grad_norm_(rnn.parameters(), args.clip)\n",
    "      optimizer.step()\n",
    "      loss_value = loss.item()\n",
    "      last_save_losses.append(loss_value)\n",
    "\n",
    "      mhx = { k : (v.detach() if isinstance(v, var) else v) for k, v in mhx.items() }#~~??\n",
    "\n",
    "        #validation\n",
    "    with torch.no_grad():\n",
    "      batch_size=args.batch_size\n",
    "      val_x,val_y,val_seq =generate_copyfirst(batch_size=batch_size, length=10,vocab_size=vocab_size)\n",
    "      #print(\"valx.size():\",val_x.size(),\" valy.size():\",val_y.size())\n",
    "      output, (chx, mhx, rv) = rnn(val_x, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "      last_output =output[:,-1,:]\n",
    "      optimizer.zero_grad()    \n",
    "      loss = criterion(last_output,val_seq[:,0])\n",
    "      #loss = criterion((last_output),val_y)\n",
    "      loss_value = loss.item()\n",
    "      last_val_losses.append(loss_value)\n",
    "\n",
    "      pred_id = torch.argmax(last_output,1) #1D\n",
    "      pred_id = pred_id.cpu().numpy() # (batch_size, vocab_len)　のはず\n",
    "      val_ansid = val_seq[:,0].cpu().numpy()\n",
    "      val_acc.append(((pred_id == val_ansid).sum()/ len(val_ansid) ))\n",
    "\n",
    "    summarize = (epoch % summarize_freq == 0)\n",
    "    if summarize:\n",
    "      loss = np.mean(last_save_losses)\n",
    "      #llprint(\"\\n\\tAvg. Logistic Loss: %.4f\\n\" % (loss))\n",
    "      if np.isnan(loss):\n",
    "        raise Exception('nan Loss')\n",
    "      print(\"epoch:\",epoch,\" loss:\",loss)\n",
    "      val_loss = np.mean(last_val_losses)\n",
    "      print(\"    val_loss:\",val_loss)\n",
    "      mean_acc =np.mean(val_acc)\n",
    "      print(\"    val_acc:\",mean_acc)\n",
    "      if mean_acc>val_bestacc:\n",
    "        val_bestacc =mean_acc\n",
    "        val_bestloss=val_loss\n",
    "        bestepoch=epoch\n",
    "        torch.save(rnn.state_dict(), save_path)\n",
    "      last_save_losses = []\n",
    "      last_val_losses =[]\n",
    "      val_acc=[]\n",
    "\n",
    "  #Test\n",
    "  print(\"=====TEST=====\")\n",
    "  last_save_losses=[]\n",
    "  accuracy_rates =[]\n",
    "  test_loss=0\n",
    "  test_acc=0\n",
    "\n",
    "  rnn.load_state_dict(torch.load(save_path))\n",
    "  rnn.eval()\n",
    "\n",
    "  '''with torch.no_grad():\n",
    "    for i,batch in enumerate(test_iter):\n",
    "      llprint(\"\\nIteration %d/%d\" % (i, iterations))\n",
    "      # We test now the learned generalization using sequence_max_length examples\n",
    "      batch_size=batch.batch_size\n",
    "      test_x,test_y,ans_id =prep_data(batch,vocab_size)\n",
    "      if val_x.size()[0]!=args.batch_size:continue\n",
    "      if rnn.debug:\n",
    "        output, (chx, mhx, rv), v = rnn(test_x, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "      else:\n",
    "        output, (chx, mhx, rv) = rnn(test_x, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "      last_output =output[:,-1,:]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(last_output,ans_id)\n",
    "      loss_value = loss.item()\n",
    "      last_save_losses.append(loss_value)\n",
    "      pred_id = torch.argmax(last_output,1) #1D\n",
    "      pred_id = pred_id.cpu().numpy() # (batch_size, vocab_len)　のはず\n",
    "      ans_id = ans_id.cpu().numpy()\n",
    "      accuracy_rates.append(((pred_id == ans_id).sum()/ len(ans_id) ))\n",
    "\n",
    "      \"\"\"try:\n",
    "        print(\"\\nReal value: \", ' = ' + str(int(target_output[0])))\n",
    "        print(\"Predicted:  \", ' = ' + str(int(output // 1)) + \" [\" + str(output) + \"]\")\n",
    "      except Exception as e:\n",
    "        pass\"\"\"\n",
    "\n",
    "    test_loss = np.mean(last_save_losses)\n",
    "    test_acc =np.mean(accuracy_rates)\n",
    "    print(\" loss:\",test_loss)\n",
    "    print(\"accuracy_rate:\",test_acc)'''\n",
    "\n",
    "  return {\"task\":task,\"best_epoch\":bestepoch,\n",
    "  \"best_val_loss\":val_bestloss,\"best_val_acc\":val_bestacc,\n",
    "  \"test_loss\":test_loss,\"test_acc\":test_acc\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5233372e-427d-463d-89fc-3ddcc6f008f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========task: 1\n",
      "batch_size: 100\n",
      "VOCAB LEN PLUS: 21\n",
      "\n",
      "----------------------------------------\n",
      "DNC(21, 256, nr_cells=256, read_heads=4, cell_size=64, gpu_id=0, independent_linears=True)\n",
      "DNC(\n",
      "  (lstm_layer_0): LSTM(277, 256, num_layers=2, batch_first=True)\n",
      "  (rnn_layer_memory_shared): Memory(\n",
      "    (read_keys_transform): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (read_strengths_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (write_key_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_strength_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (erase_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (free_gates_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (allocation_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (write_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (read_modes_transform): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=21, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "\n",
      "summarize_freq: 10\n",
      "batch 89n 0/1000epoch: 0  loss: 2.8377472109264796\n",
      "    val_loss: 6.321613788604736\n",
      "    val_acc: 0.0\n",
      "batch 89n 10/1000epoch: 10  loss: 0.6051050249983867\n",
      "    val_loss: 13.061244297027589\n",
      "    val_acc: 0.0\n",
      "batch 89n 20/1000epoch: 20  loss: 0.041085269488329586\n",
      "    val_loss: 16.601062393188478\n",
      "    val_acc: 0.0\n",
      "batch 89n 30/1000epoch: 30  loss: 0.00695305351875302\n",
      "    val_loss: 19.063822555541993\n",
      "    val_acc: 0.0\n",
      "batch 89n 40/1000epoch: 40  loss: 0.005067738944380835\n",
      "    val_loss: 20.693816566467284\n",
      "    val_acc: 0.0\n",
      "batch 89n 50/1000epoch: 50  loss: 0.019949550807755765\n",
      "    val_loss: 18.653232765197753\n",
      "    val_acc: 0.0\n",
      "batch 89n 60/1000epoch: 60  loss: 0.0980590287542307\n",
      "    val_loss: 15.786487770080566\n",
      "    val_acc: 0.002\n",
      "batch 89n 70/1000epoch: 70  loss: 0.06954454811542139\n",
      "    val_loss: 13.434070301055907\n",
      "    val_acc: 0.015\n",
      "batch 89n 80/1000epoch: 80  loss: 0.0027081093053978596\n",
      "    val_loss: 13.450911808013917\n",
      "    val_acc: 0.018\n",
      "batch 89n 90/1000epoch: 90  loss: 0.083905685446741\n",
      "    val_loss: 7.8483602868393065\n",
      "    val_acc: 0.43100000000000005\n",
      "batch 89n 100/1000epoch: 100  loss: 0.007478753140838105\n",
      "    val_loss: 0.001645729127631057\n",
      "    val_acc: 1.0\n",
      "batch 89n 110/1000epoch: 110  loss: 0.0001509227877957326\n",
      "    val_loss: 0.0004288557072868571\n",
      "    val_acc: 1.0\n",
      "batch 89n 120/1000epoch: 120  loss: 0.0015939544957023423\n",
      "    val_loss: 5.450301714517991e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 130/1000epoch: 130  loss: 0.0002414294033971092\n",
      "    val_loss: 3.385088653828916e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 140/1000epoch: 140  loss: 4.134960861691184e-05\n",
      "    val_loss: 9.299036364041058e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 150/1000epoch: 150  loss: 6.339031764958215e-08\n",
      "    val_loss: 6.55650809200381e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 160/1000epoch: 160  loss: 0.0005717586790035491\n",
      "    val_loss: 2.0116115126089796e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 170/1000epoch: 170  loss: 5.7711780608254376e-08\n",
      "    val_loss: 5.90085849072608e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 180/1000epoch: 180  loss: 0.001849338615610993\n",
      "    val_loss: 0.0020899555593422827\n",
      "    val_acc: 0.999\n",
      "batch 89n 190/1000epoch: 190  loss: 6.405229484190361e-07\n",
      "    val_loss: 1.0773857042067903e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 200/1000epoch: 200  loss: 5.339745407331422e-07\n",
      "    val_loss: 8.738010883746484e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 210/1000epoch: 210  loss: 0.005573818154588954\n",
      "    val_loss: 1.5551506613675768e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 220/1000epoch: 220  loss: 1.10221400139052e-05\n",
      "    val_loss: 0.010916126466872811\n",
      "    val_acc: 0.9950000000000001\n",
      "batch 89n 230/1000epoch: 230  loss: 3.97497207633293e-05\n",
      "    val_loss: 1.6564375583527637e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 240/1000epoch: 240  loss: 1.9762243281324426e-09\n",
      "    val_loss: 0.0021061567360794566\n",
      "    val_acc: 0.999\n",
      "batch 89n 250/1000epoch: 250  loss: 1.049944935151512e-05\n",
      "    val_loss: 2.4197213169552257e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 260/1000epoch: 260  loss: 2.0146368314340654e-09\n",
      "    val_loss: 4.7683711201784714e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 270/1000epoch: 270  loss: 0.009695518731186629\n",
      "    val_loss: 0.4299759752420759\n",
      "    val_acc: 0.9119999999999999\n",
      "batch 89n 280/1000epoch: 280  loss: 1.0722162185672345e-06\n",
      "    val_loss: 0.5534806370735168\n",
      "    val_acc: 0.8809999999999999\n",
      "batch 89n 290/1000epoch: 290  loss: 1.3511701024951052e-08\n",
      "    val_loss: 0.49630909860134126\n",
      "    val_acc: 0.8880000000000001\n",
      "batch 89n 300/1000epoch: 300  loss: 0.000183019981495424\n",
      "    val_loss: 0.3492395056411624\n",
      "    val_acc: 0.917\n",
      "batch 89n 310/1000epoch: 310  loss: 1.623942200431235e-07\n",
      "    val_loss: 0.07545619439333677\n",
      "    val_acc: 0.9440000000000002\n",
      "batch 89n 320/1000epoch: 320  loss: 0.00016329842824296516\n",
      "    val_loss: 0.01569869719865835\n",
      "    val_acc: 0.9879999999999999\n",
      "batch 89n 330/1000epoch: 330  loss: 1.887095951181984e-08\n",
      "    val_loss: 1.8691980727680856e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 340/1000epoch: 340  loss: 0.00012417467459950428\n",
      "    val_loss: 0.0012137458197504004\n",
      "    val_acc: 0.999\n",
      "batch 89n 350/1000epoch: 350  loss: 0.00015226322009648585\n",
      "    val_loss: 1.3782884804669493e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 360/1000epoch: 360  loss: 7.495437241649115e-05\n",
      "    val_loss: 1.1753564099770131e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 370/1000epoch: 370  loss: 1.8305184109242608e-09\n",
      "    val_loss: 6.997361696736704e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 380/1000epoch: 380  loss: 0.00046607996697361055\n",
      "    val_loss: 1.3589847580597336e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 390/1000epoch: 390  loss: 1.5168565436178591e-05\n",
      "    val_loss: 1.6963446825002393e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 400/1000epoch: 400  loss: 4.570676809898286e-08\n",
      "    val_loss: 3.5507390379052595e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 410/1000epoch: 410  loss: 0.0001746851580125207\n",
      "    val_loss: 1.1682506029231376e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 420/1000epoch: 420  loss: 5.43064440330612e-11\n",
      "    val_loss: 5.364417687836465e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 430/1000epoch: 430  loss: 6.10613251813182e-10\n",
      "    val_loss: 3.2186503684528133e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 440/1000epoch: 440  loss: 0.00026098997982023026\n",
      "    val_loss: 1.4185892971418922e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 450/1000epoch: 450  loss: 6.821385743914653e-09\n",
      "    val_loss: 7.15255676908555e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 460/1000epoch: 460  loss: 8.126305779722534e-05\n",
      "    val_loss: 1.7523675133190863e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 470/1000epoch: 470  loss: 8.980410484385162e-10\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 480/1000epoch: 480  loss: 1.313921244645084e-09\n",
      "    val_loss: 2.0265569888877623e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 490/1000epoch: 490  loss: 0.00016180140842701117\n",
      "    val_loss: 5.722040308242527e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 500/1000epoch: 500  loss: 9.36018182569794e-08\n",
      "    val_loss: 3.457069119860989e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 510/1000epoch: 510  loss: 5.351169139898736e-10\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 520/1000epoch: 520  loss: 0.00011065196075465768\n",
      "    val_loss: 0.0038666756429385886\n",
      "    val_acc: 0.999\n",
      "batch 89n 530/1000epoch: 530  loss: 2.679440910208836e-07\n",
      "    val_loss: 1.8596595485753653e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 540/1000epoch: 540  loss: 1.0000235743194234e-09\n",
      "    val_loss: 1.19209286886246e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 550/1000epoch: 550  loss: 9.126093966946611e-10\n",
      "    val_loss: 2.3245704316998683e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 560/1000epoch: 560  loss: 9.324652235539095e-10\n",
      "    val_loss: 7.152555658862525e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 570/1000epoch: 570  loss: 1.1920922520718907e-11\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 580/1000epoch: 580  loss: 0.0\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 590/1000epoch: 590  loss: 9.461626609151791e-05\n",
      "    val_loss: 4.768371253405235e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 600/1000epoch: 600  loss: 2.6490952641387997e-12\n",
      "    val_loss: 2.38418573772492e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 610/1000epoch: 610  loss: 0.0\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 620/1000epoch: 620  loss: 1.7563097065576017e-09\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 630/1000epoch: 630  loss: 0.0\n",
      "    val_loss: 0.0\n",
      "    val_acc: 1.0\n",
      "batch 89n 640/1000epoch: 640  loss: 1.415878066889241e-09\n",
      "    val_loss: 2.531763634294748e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 650/1000epoch: 650  loss: 6.565101815156904e-06\n",
      "    val_loss: 3.576278384542775e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 660/1000epoch: 660  loss: 1.9731734399237094e-08\n",
      "    val_loss: 2.9359234424264356e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 670/1000epoch: 670  loss: 0.00011012443070098805\n",
      "    val_loss: 9.815348056552153e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 680/1000epoch: 680  loss: 7.852841566445711e-08\n",
      "    val_loss: 4.529949333420547e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 690/1000epoch: 690  loss: 1.891453462743442e-09\n",
      "    val_loss: 3.933906222997053e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 700/1000epoch: 700  loss: 1.5648614222619884e-06\n",
      "    val_loss: 1.4066683196034547e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 710/1000epoch: 710  loss: 2.1744209010391795e-07\n",
      "    val_loss: 5.960457882814296e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 720/1000epoch: 720  loss: 2.4768274394422935e-09\n",
      "    val_loss: 7.152551928513162e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 730/1000epoch: 730  loss: 1.0146027011141238e-09\n",
      "    val_loss: 1.6689299497940624e-09\n",
      "    val_acc: 1.0\n",
      "batch 89n 740/1000epoch: 740  loss: 0.00019336572086852059\n",
      "    val_loss: 7.695054289502679e-05\n",
      "    val_acc: 1.0\n",
      "batch 89n 750/1000epoch: 750  loss: 3.966352582275413e-08\n",
      "    val_loss: 4.270052805566138e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 760/1000epoch: 760  loss: 4.2243427950462556e-07\n",
      "    val_loss: 1.0144681272805656e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 770/1000epoch: 770  loss: 4.331079063711761e-08\n",
      "    val_loss: 2.348419922970635e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 780/1000epoch: 780  loss: 1.0781785439759358e-09\n",
      "    val_loss: 1.19209286886246e-10\n",
      "    val_acc: 1.0\n",
      "batch 89n 790/1000epoch: 790  loss: 0.0017945244782543034\n",
      "    val_loss: 3.997954271284243e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 800/1000epoch: 800  loss: 3.5643543450092353e-09\n",
      "    val_loss: 3.9302378409544756e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 810/1000epoch: 810  loss: 0.0004617342635802123\n",
      "    val_loss: 1.1920921738628466e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 820/1000epoch: 820  loss: 1.0150889339458994e-08\n",
      "    val_loss: 5.8889282894547534e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 830/1000epoch: 830  loss: 7.390601726730229e-05\n",
      "    val_loss: 0.03271110954787386\n",
      "    val_acc: 0.991\n",
      "batch 89n 840/1000epoch: 840  loss: 1.1073216356195639e-09\n",
      "    val_loss: 0.0010134516105608782\n",
      "    val_acc: 1.0\n",
      "batch 89n 850/1000epoch: 850  loss: 6.05318254039607e-10\n",
      "    val_loss: 6.102706177113504e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 860/1000epoch: 860  loss: 1.945760276110696e-09\n",
      "    val_loss: 3.5285915789273756e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 870/1000epoch: 870  loss: 5.933970840625117e-10\n",
      "    val_loss: 1.418590136470499e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 880/1000epoch: 880  loss: 8.721504960565735e-05\n",
      "    val_loss: 0.09568486972730597\n",
      "    val_acc: 0.991\n",
      "batch 89n 890/1000epoch: 890  loss: 5.244770258588321e-09\n",
      "    val_loss: 2.6749978712103937e-07\n",
      "    val_acc: 1.0\n",
      "batch 89n 900/1000epoch: 900  loss: 2.6244201673828214e-08\n",
      "    val_loss: 1.3828230671641961e-08\n",
      "    val_acc: 1.0\n",
      "batch 89n 910/1000epoch: 910  loss: 3.853101267184103e-09\n",
      "    val_loss: 4.312482309254406e-06\n",
      "    val_acc: 1.0\n",
      "batch 89n 920/1000epoch: 920  loss: 0.001575729009395653\n",
      "    val_loss: 0.1938432745688507\n",
      "    val_acc: 0.986\n",
      "batch 89n 930/1000epoch: 930  loss: 1.4630780646513526e-08\n",
      "    val_loss: 0.6604501456022263\n",
      "    val_acc: 0.954\n",
      "batch 89n 940/1000epoch: 940  loss: 7.576080439284372e-06\n",
      "    val_loss: 0.4460333853596584\n",
      "    val_acc: 0.967\n",
      "batch 89n 950/1000epoch: 950  loss: 1.6521202545040206e-08\n",
      "    val_loss: 0.05742631504151632\n",
      "    val_acc: 0.992\n",
      "batch 89n 960/1000epoch: 960  loss: 1.5391230374146088e-09\n",
      "    val_loss: 0.04772692339765907\n",
      "    val_acc: 0.99\n",
      "batch 89n 970/1000epoch: 970  loss: 0.0010928630856851737\n",
      "    val_loss: 1.8938492367863629\n",
      "    val_acc: 0.8630000000000001\n",
      "batch 89n 980/1000epoch: 980  loss: 2.1629860571772656e-09\n",
      "    val_loss: 2.139605188369751\n",
      "    val_acc: 0.8230000000000001\n",
      "batch 89n 990/1000epoch: 990  loss: 0.0005107973594929\n",
      "    val_loss: 0.938547670841217\n",
      "    val_acc: 0.8710000000000001\n",
      "batch 89n 1000/1000epoch: 1000  loss: 0.000272102982962746\n",
      "    val_loss: 2.3614335656166077\n",
      "    val_acc: 0.8360000000000001\n",
      "=====TEST=====\n",
      "==========task: 2\n",
      "batch_size: 100\n",
      "VOCAB LEN PLUS: 35\n",
      "\n",
      "----------------------------------------\n",
      "DNC(35, 256, nr_cells=256, read_heads=4, cell_size=64, gpu_id=0, independent_linears=True)\n",
      "DNC(\n",
      "  (lstm_layer_0): LSTM(291, 256, num_layers=2, batch_first=True)\n",
      "  (rnn_layer_memory_shared): Memory(\n",
      "    (read_keys_transform): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (read_strengths_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (write_key_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_strength_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (erase_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (free_gates_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (allocation_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (write_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (read_modes_transform): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=35, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "\n",
      "summarize_freq: 10\n",
      "batch 89n 0/1000epoch: 0  loss: 3.145609810617235\n",
      "    val_loss: 2.744041681289673\n",
      "    val_acc: 0.11\n",
      "batch 89n 10/1000epoch: 10  loss: 1.2917059518562424\n",
      "    val_loss: 5.185504269599915\n",
      "    val_acc: 0.009999999999999998\n",
      "batch 89n 20/1000epoch: 20  loss: 0.1692145227873698\n",
      "    val_loss: 7.927964925765991\n",
      "    val_acc: 0.036\n",
      "batch 89n 30/1000epoch: 30  loss: 0.043527997861383484\n",
      "    val_loss: 8.070661926269532\n",
      "    val_acc: 0.08600000000000001\n",
      "batch 89n 40/1000epoch: 40  loss: 0.010819705159948272\n",
      "    val_loss: 8.05622239112854\n",
      "    val_acc: 0.119\n",
      "batch 89n 50/1000epoch: 50  loss: 0.015542418672711292\n",
      "    val_loss: 8.082971668243408\n",
      "    val_acc: 0.183\n",
      "batch 89n 60/1000epoch: 60  loss: 0.009068399268027051\n",
      "    val_loss: 7.273994731903076\n",
      "    val_acc: 0.21400000000000002\n",
      "batch 89n 70/1000epoch: 70  loss: 0.01792397830841259\n",
      "    val_loss: 5.052590179443359\n",
      "    val_acc: 0.3390000000000001\n",
      "batch 89n 80/1000epoch: 80  loss: 0.013528229111640637\n",
      "    val_loss: 6.434011936187744\n",
      "    val_acc: 0.354\n",
      "batch 89n 90/1000epoch: 90  loss: 0.004909184982983182\n",
      "    val_loss: 5.143317222595215\n",
      "    val_acc: 0.4040000000000001\n",
      "batch 89n 100/1000epoch: 100  loss: 0.004949864628848041\n",
      "    val_loss: 4.151137208938598\n",
      "    val_acc: 0.48100000000000004\n",
      "batch 89n 110/1000epoch: 110  loss: 0.002554653001912407\n",
      "    val_loss: 4.88866446018219\n",
      "    val_acc: 0.487\n",
      "batch 89n 120/1000epoch: 120  loss: 0.0013485175386267656\n",
      "    val_loss: 5.459511804580688\n",
      "    val_acc: 0.403\n",
      "batch 89n 130/1000epoch: 130  loss: 0.0023027323080360576\n",
      "    val_loss: 4.519979476928711\n",
      "    val_acc: 0.5170000000000001\n",
      "batch 89n 140/1000epoch: 140  loss: 2.58602430007847e-06\n",
      "    val_loss: 3.4510931968688965\n",
      "    val_acc: 0.631\n",
      "batch 89n 150/1000epoch: 150  loss: 0.0006673095302408336\n",
      "    val_loss: 5.013510894775391\n",
      "    val_acc: 0.565\n",
      "batch 89n 160/1000epoch: 160  loss: 0.0019047041665994078\n",
      "    val_loss: 7.215295457839966\n",
      "    val_acc: 0.36\n",
      "batch 89n 170/1000epoch: 170  loss: 0.0018111785858748893\n",
      "    val_loss: 7.351253175735474\n",
      "    val_acc: 0.38599999999999995\n",
      "batch 89n 180/1000epoch: 180  loss: 0.019261986209923615\n",
      "    val_loss: 5.817978262901306\n",
      "    val_acc: 0.442\n",
      "batch 89n 190/1000epoch: 190  loss: 7.824064408339381e-06\n",
      "    val_loss: 5.642953181266785\n",
      "    val_acc: 0.438\n",
      "batch 89n 200/1000epoch: 200  loss: 0.0037410352197549124\n",
      "    val_loss: 1.3440714836120606\n",
      "    val_acc: 0.806\n",
      "batch 89n 210/1000epoch: 210  loss: 1.5508721472359513e-06\n",
      "    val_loss: 1.2706381559371949\n",
      "    val_acc: 0.8390000000000001\n",
      "batch 89n 220/1000epoch: 220  loss: 3.0706636235702984e-05\n",
      "    val_loss: 1.4027868509292603\n",
      "    val_acc: 0.7819999999999999\n",
      "batch 89n 230/1000epoch: 230  loss: 2.4963541216147274e-08\n",
      "    val_loss: 2.4864309549331667\n",
      "    val_acc: 0.6910000000000001\n",
      "batch 89n 240/1000epoch: 240  loss: 0.0016349513544551073\n",
      "    val_loss: 4.3942462682724\n",
      "    val_acc: 0.583\n",
      "batch 89n 250/1000epoch: 250  loss: 0.0007291646318439307\n",
      "    val_loss: 4.288641881942749\n",
      "    val_acc: 0.562\n",
      "batch 89n 260/1000epoch: 260  loss: 8.153789107604818e-08\n",
      "    val_loss: 3.074850630760193\n",
      "    val_acc: 0.673\n",
      "batch 89n 270/1000epoch: 270  loss: 0.00018348057897764095\n",
      "    val_loss: 5.626210403442383\n",
      "    val_acc: 0.574\n",
      "batch 89n 280/1000epoch: 280  loss: 0.0002500460692058947\n",
      "    val_loss: 9.337153816223145\n",
      "    val_acc: 0.3509999999999999\n",
      "batch 89n 290/1000epoch: 290  loss: 0.004344798892196254\n",
      "    val_loss: 4.326723265647888\n",
      "    val_acc: 0.616\n",
      "batch 89n 300/1000epoch: 300  loss: 5.316288519689275e-07\n",
      "    val_loss: 2.798431956768036\n",
      "    val_acc: 0.7539999999999999\n",
      "batch 89n 310/1000epoch: 310  loss: 8.336124340344339e-08\n",
      "    val_loss: 1.822675609588623\n",
      "    val_acc: 0.776\n",
      "batch 89n 320/1000epoch: 320  loss: 0.000730845520085054\n",
      "    val_loss: 4.114906477928161\n",
      "    val_acc: 0.5860000000000001\n",
      "batch 89n 330/1000epoch: 330  loss: 0.0002184062914498952\n",
      "    val_loss: 3.7495937824249266\n",
      "    val_acc: 0.679\n",
      "batch 89n 340/1000epoch: 340  loss: 5.275809144000831e-08\n",
      "    val_loss: 3.7145760536193846\n",
      "    val_acc: 0.6759999999999999\n",
      "batch 89n 350/1000epoch: 350  loss: 0.009861559591941533\n",
      "    val_loss: 6.304216098785401\n",
      "    val_acc: 0.623\n",
      "batch 89n 360/1000epoch: 360  loss: 8.183609028628149e-06\n",
      "    val_loss: 16.400791549682616\n",
      "    val_acc: 0.152\n",
      "batch 89n 370/1000epoch: 370  loss: 7.315989612220896e-08\n",
      "    val_loss: 16.39354724884033\n",
      "    val_acc: 0.196\n",
      "batch 89n 380/1000epoch: 380  loss: 7.540209492463824e-08\n",
      "    val_loss: 13.637031841278077\n",
      "    val_acc: 0.19\n",
      "batch 89n 390/1000epoch: 390  loss: 2.7463359575374825e-08\n",
      "    val_loss: 13.516030883789062\n",
      "    val_acc: 0.231\n",
      "batch 89n 400/1000epoch: 400  loss: 0.00016072404480500736\n",
      "    val_loss: 9.392494106292725\n",
      "    val_acc: 0.35500000000000004\n",
      "batch 89n 410/1000epoch: 410  loss: 6.3001517202436e-08\n",
      "    val_loss: 7.164496040344238\n",
      "    val_acc: 0.41400000000000003\n",
      "batch 89n 420/1000epoch: 420  loss: 1.429852159893219e-06\n",
      "    val_loss: 9.842445707321167\n",
      "    val_acc: 0.39699999999999996\n",
      "batch 89n 430/1000epoch: 430  loss: 0.00010830893817716378\n",
      "    val_loss: 10.998754978179932\n",
      "    val_acc: 0.33499999999999996\n",
      "batch 89n 440/1000epoch: 440  loss: 0.00015589265300533666\n",
      "    val_loss: 8.733489608764648\n",
      "    val_acc: 0.45099999999999996\n",
      "batch 89n 450/1000epoch: 450  loss: 9.865938924306219e-06\n",
      "    val_loss: 8.96595540046692\n",
      "    val_acc: 0.453\n",
      "batch 89n 460/1000epoch: 460  loss: 0.0002099994339562968\n",
      "    val_loss: 9.579263973236085\n",
      "    val_acc: 0.40099999999999997\n",
      "batch 89n 470/1000epoch: 470  loss: 0.0004092610247563227\n",
      "    val_loss: 15.699923419952393\n",
      "    val_acc: 0.27999999999999997\n",
      "batch 89n 480/1000epoch: 480  loss: 5.29686484525745e-09\n",
      "    val_loss: 14.958189296722413\n",
      "    val_acc: 0.273\n",
      "batch 89n 490/1000epoch: 490  loss: 4.4575881057637745e-07\n",
      "    val_loss: 15.487744617462159\n",
      "    val_acc: 0.21999999999999997\n",
      "batch 89n 500/1000epoch: 500  loss: 0.00023132120903625485\n",
      "    val_loss: 16.765738105773927\n",
      "    val_acc: 0.27599999999999997\n",
      "batch 89n 510/1000epoch: 510  loss: 2.6738806222010366e-08\n",
      "    val_loss: 18.23698139190674\n",
      "    val_acc: 0.21400000000000002\n",
      "batch 89n 520/1000epoch: 520  loss: 1.6552661160596195e-05\n",
      "    val_loss: 15.49357566833496\n",
      "    val_acc: 0.364\n",
      "batch 89n 530/1000epoch: 530  loss: 7.936684156057478e-09\n",
      "    val_loss: 6.628721690177917\n",
      "    val_acc: 0.6759999999999999\n",
      "batch 89n 540/1000epoch: 540  loss: 9.722164451275717e-10\n",
      "    val_loss: 7.779056978225708\n",
      "    val_acc: 0.6199999999999999\n",
      "batch 89n 550/1000epoch: 550  loss: 5.2703321628818264e-05\n",
      "    val_loss: 10.241002178192138\n",
      "    val_acc: 0.45899999999999996\n",
      "batch 89n 560/1000epoch: 560  loss: 0.00018477636248435126\n",
      "    val_loss: 9.632864809036255\n",
      "    val_acc: 0.48100000000000004\n",
      "batch 89n 570/1000epoch: 570  loss: 1.9854965481612573e-09\n",
      "    val_loss: 7.33948917388916\n",
      "    val_acc: 0.5660000000000001\n",
      "batch 89n 580/1000epoch: 580  loss: 1.1282883016858511e-06\n",
      "    val_loss: 6.977764368057251\n",
      "    val_acc: 0.568\n",
      "batch 89n 590/1000epoch: 590  loss: 6.993595140040535e-10\n",
      "    val_loss: 6.970617294311523\n",
      "    val_acc: 0.5619999999999999\n",
      "batch 89n 600/1000epoch: 600  loss: 0.0003922279033462275\n",
      "    val_loss: 5.327049612998962\n",
      "    val_acc: 0.667\n",
      "batch 89n 610/1000epoch: 610  loss: 6.781631515610324e-10\n",
      "    val_loss: 4.020664811134338\n",
      "    val_acc: 0.758\n",
      "batch 89n 620/1000epoch: 620  loss: 7.536673043675693e-10\n",
      "    val_loss: 4.081454396247864\n",
      "    val_acc: 0.6759999999999999\n",
      "batch 89n 630/1000epoch: 630  loss: 0.0007781707457939029\n",
      "    val_loss: 5.233778095245361\n",
      "    val_acc: 0.6759999999999999\n",
      "batch 89n 640/1000epoch: 640  loss: 6.792274286719172e-09\n",
      "    val_loss: 4.840089797973633\n",
      "    val_acc: 0.6619999999999999\n",
      "batch 89n 650/1000epoch: 650  loss: 6.411483945916668e-07\n",
      "    val_loss: 6.120324230194091\n",
      "    val_acc: 0.6159999999999999\n",
      "batch 89n 660/1000epoch: 660  loss: 0.0004554378841708215\n",
      "    val_loss: 3.8185009241104124\n",
      "    val_acc: 0.696\n",
      "batch 89n 670/1000epoch: 670  loss: 4.727141202554463e-05\n",
      "    val_loss: 3.240529602766037\n",
      "    val_acc: 0.744\n",
      "batch 89n 680/1000epoch: 680  loss: 1.3935538399037187e-08\n",
      "    val_loss: 3.2373696088790895\n",
      "    val_acc: 0.716\n",
      "batch 89n 690/1000epoch: 690  loss: 4.595719016063803e-07\n",
      "    val_loss: 3.630017614364624\n",
      "    val_acc: 0.7230000000000001\n",
      "batch 89n 700/1000epoch: 700  loss: 3.9273347678130895e-05\n",
      "    val_loss: 4.54212498664856\n",
      "    val_acc: 0.758\n",
      "batch 89n 710/1000epoch: 710  loss: 1.6532319360064286e-07\n",
      "    val_loss: 4.544457793235779\n",
      "    val_acc: 0.768\n",
      "batch 89n 720/1000epoch: 720  loss: 8.934142074884521e-05\n",
      "    val_loss: 3.105684280395508\n",
      "    val_acc: 0.7789999999999999\n",
      "batch 89n 730/1000epoch: 730  loss: 6.17237867730738e-10\n",
      "    val_loss: 3.1838134050369264\n",
      "    val_acc: 0.756\n",
      "batch 89n 740/1000epoch: 740  loss: 3.0464589369690503e-11\n",
      "    val_loss: 2.4439720034599306\n",
      "    val_acc: 0.775\n",
      "batch 89n 750/1000epoch: 750  loss: 0.00018450830599015767\n",
      "    val_loss: 2.4525729179382325\n",
      "    val_acc: 0.8049999999999999\n",
      "batch 89n 760/1000epoch: 760  loss: 4.9525442541566003e-08\n",
      "    val_loss: 2.223135995864868\n",
      "    val_acc: 0.8049999999999999\n",
      "batch 89n 770/1000epoch: 770  loss: 1.8330899336720878e-09\n",
      "    val_loss: 1.8786579608917235\n",
      "    val_acc: 0.825\n",
      "batch 89n 780/1000epoch: 780  loss: 0.0008223923793688566\n",
      "    val_loss: 2.3383946061134337\n",
      "    val_acc: 0.8710000000000001\n",
      "batch 89n 790/1000epoch: 790  loss: 8.056180297747832e-07\n",
      "    val_loss: 1.5915940999984741\n",
      "    val_acc: 0.891\n",
      "batch 89n 800/1000epoch: 800  loss: 4.4027852618021435e-09\n",
      "    val_loss: 1.4234392076730729\n",
      "    val_acc: 0.905\n",
      "batch 89n 810/1000epoch: 810  loss: 2.815158981794852e-07\n",
      "    val_loss: 1.839023905992508\n",
      "    val_acc: 0.9059999999999999\n",
      "batch 89n 820/1000epoch: 820  loss: 5.862773631195584e-06\n",
      "    val_loss: 2.4377816379070283\n",
      "    val_acc: 0.842\n",
      "batch 89n 830/1000epoch: 830  loss: 0.0007827081113458445\n",
      "    val_loss: 4.023880672454834\n",
      "    val_acc: 0.76\n",
      "batch 89n 840/1000epoch: 840  loss: 2.9245934950338103e-09\n",
      "    val_loss: 2.2343034982681274\n",
      "    val_acc: 0.8879999999999999\n",
      "batch 89n 850/1000epoch: 850  loss: 0.00021920136693906727\n",
      "    val_loss: 1.3091905921697617\n",
      "    val_acc: 0.924\n",
      "batch 89n 860/1000epoch: 860  loss: 3.5729996168163167e-08\n",
      "    val_loss: 0.878789946436882\n",
      "    val_acc: 0.9380000000000001\n",
      "batch 89n 870/1000epoch: 870  loss: 7.669127607042488e-10\n",
      "    val_loss: 0.675655472278595\n",
      "    val_acc: 0.9339999999999999\n",
      "batch 89n 880/1000epoch: 880  loss: 0.0009945852457615301\n",
      "    val_loss: 1.7138859394937753\n",
      "    val_acc: 0.89\n",
      "batch 89n 890/1000epoch: 890  loss: 9.951432345032016e-08\n",
      "    val_loss: 3.798469614982605\n",
      "    val_acc: 0.762\n",
      "batch 89n 900/1000epoch: 900  loss: 1.4079939155327754e-09\n",
      "    val_loss: 3.5951993346214293\n",
      "    val_acc: 0.7550000000000001\n",
      "batch 89n 910/1000epoch: 910  loss: 1.553684758132729e-09\n",
      "    val_loss: 2.2664405584335325\n",
      "    val_acc: 0.833\n",
      "batch 89n 920/1000epoch: 920  loss: 2.6210535991677554e-06\n",
      "    val_loss: 0.7044731810688972\n",
      "    val_acc: 0.9549999999999998\n",
      "batch 89n 930/1000epoch: 930  loss: 3.8565310868222804e-09\n",
      "    val_loss: 2.017081091552973\n",
      "    val_acc: 0.9090000000000001\n",
      "batch 89n 940/1000epoch: 940  loss: 0.0001353804768568535\n",
      "    val_loss: 4.382885837554932\n",
      "    val_acc: 0.849\n",
      "batch 89n 950/1000epoch: 950  loss: 1.5959166365073527e-07\n",
      "    val_loss: 5.1730067253112795\n",
      "    val_acc: 0.817\n",
      "batch 89n 960/1000epoch: 960  loss: 7.947285545700171e-11\n",
      "    val_loss: 4.775135064125061\n",
      "    val_acc: 0.819\n",
      "batch 89n 970/1000epoch: 970  loss: 0.0001365345315040065\n",
      "    val_loss: 3.9260396480560305\n",
      "    val_acc: 0.825\n",
      "batch 89n 980/1000epoch: 980  loss: 4.9349938654833874e-08\n",
      "    val_loss: 3.03756902217865\n",
      "    val_acc: 0.8300000000000001\n",
      "batch 89n 990/1000epoch: 990  loss: 2.0133033832673657e-10\n",
      "    val_loss: 2.9023892760276793\n",
      "    val_acc: 0.859\n",
      "batch 89n 1000/1000epoch: 1000  loss: 2.442795880335469e-05\n",
      "    val_loss: 3.6811251878738402\n",
      "    val_acc: 0.8150000000000001\n",
      "=====TEST=====\n",
      "==========task: 3\n",
      "batch_size: 100\n",
      "VOCAB LEN PLUS: 36\n",
      "\n",
      "----------------------------------------\n",
      "DNC(36, 256, nr_cells=256, read_heads=4, cell_size=64, gpu_id=0, independent_linears=True)\n",
      "DNC(\n",
      "  (lstm_layer_0): LSTM(292, 256, num_layers=2, batch_first=True)\n",
      "  (rnn_layer_memory_shared): Memory(\n",
      "    (read_keys_transform): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (read_strengths_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (write_key_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_strength_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (erase_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (write_vector_transform): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (free_gates_transform): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (allocation_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (write_gate_transform): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (read_modes_transform): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=36, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "\n",
      "summarize_freq: 10\n",
      "batch 89n 0/1000epoch: 0  loss: 3.0897230439715915\n",
      "    val_loss: 3.3199398517608643\n",
      "    val_acc: 0.04\n",
      "batch 89n 10/1000epoch: 10  loss: 0.8487852091424996\n",
      "    val_loss: 10.644299697875976\n",
      "    val_acc: 0.0\n",
      "batch 89n 20/1000epoch: 20  loss: 0.8987321638067564\n",
      "    val_loss: 12.192977237701417\n",
      "    val_acc: 0.0\n",
      "batch 89n 30/1000epoch: 30  loss: 0.6929459206428793\n",
      "    val_loss: 13.005780696868896\n",
      "    val_acc: 0.0\n",
      "batch 89n 40/1000epoch: 40  loss: 0.2324387290939275\n",
      "    val_loss: 15.582120037078857\n",
      "    val_acc: 0.018000000000000002\n",
      "batch 89n 50/1000epoch: 50  loss: 0.2936071117263701\n",
      "    val_loss: 11.276474046707154\n",
      "    val_acc: 0.10600000000000001\n",
      "batch 89n 60/1000epoch: 60  loss: 0.09636742339986894\n",
      "    val_loss: 7.190783214569092\n",
      "    val_acc: 0.15899999999999997\n",
      "batch 89n 70/1000epoch: 70  loss: 0.03751169793970702\n",
      "    val_loss: 5.044579410552979\n",
      "    val_acc: 0.36900000000000005\n",
      "batch 25n 71/1000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m state\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m21\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     state \u001b[38;5;241m.\u001b[39mappend(\u001b[43mmain_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame(state))\n",
      "Cell \u001b[0;32mIn [12], line 97\u001b[0m, in \u001b[0;36mmain_copy\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#print(\"last_output.size():\",last_output.size(),\" ans_id.size():\",ans_id.size())\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#loss = criterion((last_output),train_y)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(last_output,sequence[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m T\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(rnn\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mclip)\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    131\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[1;32m    132\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state=[]\n",
    "for task in range(1,21):\n",
    "    state .append(main_copy(task))\n",
    "\n",
    "print(pd.DataFrame(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe49def",
   "metadata": {},
   "source": [
    "criterion  \n",
    "clip_grad, detach  \n",
    "＝＝↑もうやった  \n",
    "やっぱりまずはデータ前処理。SAMやDNTM＋とかはpytorchだし、前処理に関しては正直tfのでもいい  \n",
    "てかもうSAMでよくない?  \n",
    "dnc  \n",
    "やっぱり最初に、2021以降くらいのメモリモデルのコード探そう  \n",
    "無さそうなんだが　メモリネットのpytorch版あるからこれ動かそう  \n",
    "copyfirstは上手く行くんだし、supporting factなど前処理 これをまずやってみる  \n",
    "parameter, モデルの引数。  \n",
    "　コードを読み返して、操作可能パラメータをどこかにまとめるのが良い。その後、SAMなど発展手法の論文に書いてないかさがす  \n",
    "新しい方へ：scholarみて引用数多い発展手法のソースコードと比較  \n",
    "　pytorch実装の望みがありそう  \n",
    "　H-Memはあった  \n",
    "古い方へ：memory networkなどの論文を読んで、それのソースコードを動かす。そっちのほうが簡単なはず。  \n",
    "　なんならDNCじゃなくてもっと変なことしてもいいので\n",
    "　古いモデル：NTM, dynamic NTM, E2E mem, memory network  \n",
    "　pytorchがある場合のみでいい。tfを扱うのはもっとあと  \n",
    "もしくはtfのコードと比較  \n",
    "パラメータなどを可視化する。しんどそうだけど、どのみちやるので  \n",
    "  \n",
    " \n",
    "ちょうどSAMの論文に前処理のとこがあった　他にもあると思うけど  \n",
    "(inputとか　合ってる？)  \n",
    "  \n",
    "11までになんとかならなかったら、tfに変える。とりあえず動くことが最優先  \n",
    "となると8,9,10でtfモデルとの比較をしないといけない。それで解決しなかったら。  \n",
    "でもパラメータの可視化とかもしたいし、わからん"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d3ce7",
   "metadata": {},
   "source": [
    "detachのとことかまだよくわかってない  \n",
    "lstmはモデルの外に隠れ状態を出していなかった。いや出力として出てくるんだけど、モデルの入力として渡さない。この場合detachをしなくて問題なかった  \n",
    "こちらではモデルの出力として出したものを、次に入力に入れる。この場合計算グラフをさかのぼっていく？でも↑の場合だと遡る？それともライブラリの方で値引き継ぎとdetachをする？  \n",
    "あとこっちの場合、chxとかを入力にしなくてもいいのか気になる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce411fc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

\chapter{関連研究}

\section{Neural Turing Machine}
NTMの構造は3つのコンポーネントに分けられる。
\\１．情報を保存するメモリ$M_t$は$N×W$次元の行列からなり、これは$W$次元の項目を$N$スロット分保存できる。
\\２．コントローラは毎時間ステップで入力$x_t$を受け取り、隠れ状態$h_t$を計算する。
任意のRNNがコントローラとして採用可能だが、大抵LSTMが用いられる。
\\３．読み出し/書き込みヘッドはコントローラ出力$h_t$とメモリ$M_t$の内容から読み出し/書き込み重みを計算し、メモリへの読み書きを行う。
読み出し/書き込み重みはそれぞれ各スロットへの書き込み/読み出しの強度を表す$N$次元のアテンション係数である。
この重みを計算する操作を”アドレス指定”操作と呼ぶ。

2.1.1節ではヘッドにおける読み書きを説明する。2.1.2節ではアドレス指定を説明する。

\subsection{読み出し/書き込みヘッド}
NTMは読み出し用のヘッドと書き込み用のヘッドそれぞれを1つ以上持ち、ヘッドごとにアドレス指定及び読み書きが行われる。
読み/書き重みをそれぞれ$w^r_t$,$w^w_t$と表し、これらは2.1.2節で説明するアドレス指定操作を用いて計算される。
$w^r_t$,$w^w_t$は式～、～を満たすアテンション係数である。
\begin{equation}
	\sum_{i}w_t(i) = 1
\end{equation}
\begin{equation}
	0\leq w_t(i)\leq 1 , \forall i
\end{equation}
ここで$w_t(i)$は$w_t$のi番目の要素である。メモリからの読み出しは式～のように計算され、読み出しベクトル$r_t$を得る。
\begin{equation}
	r_t = \sum_{i}w_t(i)M_t(i)
\end{equation}
$M_t(i)$は$M_t$のi行目を表す。書き込みは式～による忘却、式～による加算の順で計算され、$M_{t-1}$を$M_t$に更新する。
忘却ベクトル$e_t$、書き込みベクトル$a_t$はコントローラ出力からの変換で得られる。ただし忘却ベクトルの各要素は(0,1)の範囲にある。
\begin{equation}
	M_t'(i) = M_{t-1}(i)[1-w_t(i)e_t]
\end{equation}
\begin{equation}
	M_t(i) = M_t'(i) + w_t(i)a_t
\end{equation}

\subsection{アドレス指定操作}

\section{Relational Memory Core}
関係メモリのベースとして使用した、[R-RNN]で提案されたRMCを説明する。
RMCは毎ステップで入力$x_t$とメモリ$M_{t-1}$の各項目間の関係情報を計算し、そのステップでのメモリ成分$\tilde{M}$を用意する。
LSTMベースのゲーティングを利用して、$\tilde{M}$を$M_{t-1}$に合成し$M_t$を得る。
$\tilde{M}$の計算はマルチヘッドドット積アテンション[Transformer]を用いて行われる。
RMCは項目メモリ$M_i$と入力$x_t$からアテンションを計算するために、クエリ・キー・バリューを計算する為の訓練可能な線形層を有する。
それぞれを$W_q$,$W_k$,$W_v$と表現すると、$\tilde{M}$は式～のようにして計算される。
\begin{equation}
	\tilde{M} =softmax ( \frac{M_{t-1}W_q ([M_{t-1};x_t]W_k)^T}{ \sqrt{d_k}} ) [M_{t-1};x_t]W_v
\end{equation}
ここで$d_k$はキーベクトルの次元　、[M;x]は$M$に$x_t$を新たな行として連結した$(N+1)\times M$次元の行列を表す。クエリ行列の計算では[M:x]ではなく$M$を入力することに注意が必要である。これは$\tilde{M}$の次元を$M_t$と等しくすることを目的としている。

ヘッドが複数存在する時は、ヘッドごとに独立な線形層を用いたアテンション計算結果を結合し最終的な$\tilde{M}$を得る。
各ヘッドの計算結果を$\tilde{M_1},\tilde{M_2}…\tilde{M_h}$と表すとき、それぞれの次元は$N\times (M/h)$であり、$\tilde{M_t}=[\tilde{M_1}: … :\tilde{M_h}]$とすることで$M_t$と同じ次元の$\tilde{M}$を得る。
[:]は行方向の連結を表す。

$\tilde{M}$により$M_t$を更新するためにLSTMを利用する。$M_t$の各行を2D-LSTMの各メモリセルとして実装することで、式x～yによって更新される。$m_i$,$\tilde{m_i}$はそれぞれ$M_t$,$\tilde{M}$の$i$番目の行を表す。
\begin{equation}
	 =
\end{equation}
\begin{equation}
	=
\end{equation}
\begin{equation}
	=
\end{equation}
式～において下線部が示す箇所はLSTMからの変更部分である。関数gは既存研究[R-RNN]に従い、MLP + layer normalizationとして実装した。パラメータ は各$m_i$について共通する。
